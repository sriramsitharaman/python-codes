{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\sriram\\\\Desktop\\\\Kaggle\\\\Two - Sigma\\\\Data\\\\\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "# adding all these new features to use list #\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10                                                         \n",
      "10000     Doorman Elevator Fitness_Center Cats_Allowed D...\n",
      "100004    Laundry_In_Building Dishwasher Hardwood_Floors...\n",
      "100007                               Hardwood_Floors No_Fee\n",
      "100013                                              Pre-War\n",
      "Name: features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "print(train_df[\"features\"].head())\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-83c010279db7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'unique'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m             \u001b[1;31m# to return array of Timestamp with tz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    971\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munique1d\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36munique1d\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_hash\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPyObjectHashTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ensure_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.unique (pandas\\hashtable.c:14990)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 217) (74659, 217)\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.03971\ttest-mlogloss:1.03938\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:0.988689\ttest-mlogloss:0.988538\n",
      "[2]\ttrain-mlogloss:0.944997\ttest-mlogloss:0.945176\n",
      "[3]\ttrain-mlogloss:0.90621\ttest-mlogloss:0.906935\n",
      "[4]\ttrain-mlogloss:0.87539\ttest-mlogloss:0.876627\n",
      "[5]\ttrain-mlogloss:0.845565\ttest-mlogloss:0.847241\n",
      "[6]\ttrain-mlogloss:0.821469\ttest-mlogloss:0.823368\n",
      "[7]\ttrain-mlogloss:0.799178\ttest-mlogloss:0.801567\n",
      "[8]\ttrain-mlogloss:0.779889\ttest-mlogloss:0.782733\n",
      "[9]\ttrain-mlogloss:0.760895\ttest-mlogloss:0.764206\n",
      "[10]\ttrain-mlogloss:0.744347\ttest-mlogloss:0.748191\n",
      "[11]\ttrain-mlogloss:0.731749\ttest-mlogloss:0.736026\n",
      "[12]\ttrain-mlogloss:0.719089\ttest-mlogloss:0.723851\n",
      "[13]\ttrain-mlogloss:0.706964\ttest-mlogloss:0.712215\n",
      "[14]\ttrain-mlogloss:0.695924\ttest-mlogloss:0.701853\n",
      "[15]\ttrain-mlogloss:0.686835\ttest-mlogloss:0.693334\n",
      "[16]\ttrain-mlogloss:0.678381\ttest-mlogloss:0.685522\n",
      "[17]\ttrain-mlogloss:0.671025\ttest-mlogloss:0.678703\n",
      "[18]\ttrain-mlogloss:0.663785\ttest-mlogloss:0.672112\n",
      "[19]\ttrain-mlogloss:0.657343\ttest-mlogloss:0.66617\n",
      "[20]\ttrain-mlogloss:0.649914\ttest-mlogloss:0.659444\n",
      "[21]\ttrain-mlogloss:0.643862\ttest-mlogloss:0.654005\n",
      "[22]\ttrain-mlogloss:0.637932\ttest-mlogloss:0.64892\n",
      "[23]\ttrain-mlogloss:0.631824\ttest-mlogloss:0.643418\n",
      "[24]\ttrain-mlogloss:0.626251\ttest-mlogloss:0.638473\n",
      "[25]\ttrain-mlogloss:0.621689\ttest-mlogloss:0.63447\n",
      "[26]\ttrain-mlogloss:0.617366\ttest-mlogloss:0.6306\n",
      "[27]\ttrain-mlogloss:0.613768\ttest-mlogloss:0.627385\n",
      "[28]\ttrain-mlogloss:0.60994\ttest-mlogloss:0.624084\n",
      "[29]\ttrain-mlogloss:0.606484\ttest-mlogloss:0.621123\n",
      "[30]\ttrain-mlogloss:0.604089\ttest-mlogloss:0.619131\n",
      "[31]\ttrain-mlogloss:0.600982\ttest-mlogloss:0.61653\n",
      "[32]\ttrain-mlogloss:0.597937\ttest-mlogloss:0.614154\n",
      "[33]\ttrain-mlogloss:0.594586\ttest-mlogloss:0.611228\n",
      "[34]\ttrain-mlogloss:0.59142\ttest-mlogloss:0.608799\n",
      "[35]\ttrain-mlogloss:0.588855\ttest-mlogloss:0.606946\n",
      "[36]\ttrain-mlogloss:0.58701\ttest-mlogloss:0.605535\n",
      "[37]\ttrain-mlogloss:0.584449\ttest-mlogloss:0.603581\n",
      "[38]\ttrain-mlogloss:0.581861\ttest-mlogloss:0.601424\n",
      "[39]\ttrain-mlogloss:0.579449\ttest-mlogloss:0.599633\n",
      "[40]\ttrain-mlogloss:0.576873\ttest-mlogloss:0.597856\n",
      "[41]\ttrain-mlogloss:0.574634\ttest-mlogloss:0.596299\n",
      "[42]\ttrain-mlogloss:0.572442\ttest-mlogloss:0.59489\n",
      "[43]\ttrain-mlogloss:0.570574\ttest-mlogloss:0.593715\n",
      "[44]\ttrain-mlogloss:0.569298\ttest-mlogloss:0.592768\n",
      "[45]\ttrain-mlogloss:0.567664\ttest-mlogloss:0.591636\n",
      "[46]\ttrain-mlogloss:0.565811\ttest-mlogloss:0.590567\n",
      "[47]\ttrain-mlogloss:0.563834\ttest-mlogloss:0.5892\n",
      "[48]\ttrain-mlogloss:0.561876\ttest-mlogloss:0.58821\n",
      "[49]\ttrain-mlogloss:0.560155\ttest-mlogloss:0.587143\n",
      "[50]\ttrain-mlogloss:0.558551\ttest-mlogloss:0.586098\n",
      "[51]\ttrain-mlogloss:0.556584\ttest-mlogloss:0.58528\n",
      "[52]\ttrain-mlogloss:0.554673\ttest-mlogloss:0.584066\n",
      "[53]\ttrain-mlogloss:0.552928\ttest-mlogloss:0.583299\n",
      "[54]\ttrain-mlogloss:0.55118\ttest-mlogloss:0.582179\n",
      "[55]\ttrain-mlogloss:0.549648\ttest-mlogloss:0.581046\n",
      "[56]\ttrain-mlogloss:0.548239\ttest-mlogloss:0.580178\n",
      "[57]\ttrain-mlogloss:0.54703\ttest-mlogloss:0.579449\n",
      "[58]\ttrain-mlogloss:0.545845\ttest-mlogloss:0.578863\n",
      "[59]\ttrain-mlogloss:0.544711\ttest-mlogloss:0.578275\n",
      "[60]\ttrain-mlogloss:0.543561\ttest-mlogloss:0.577673\n",
      "[61]\ttrain-mlogloss:0.54201\ttest-mlogloss:0.576803\n",
      "[62]\ttrain-mlogloss:0.540603\ttest-mlogloss:0.575997\n",
      "[63]\ttrain-mlogloss:0.539258\ttest-mlogloss:0.57539\n",
      "[64]\ttrain-mlogloss:0.53769\ttest-mlogloss:0.57479\n",
      "[65]\ttrain-mlogloss:0.536582\ttest-mlogloss:0.574289\n",
      "[66]\ttrain-mlogloss:0.535222\ttest-mlogloss:0.573662\n",
      "[67]\ttrain-mlogloss:0.533949\ttest-mlogloss:0.573015\n",
      "[68]\ttrain-mlogloss:0.532956\ttest-mlogloss:0.572444\n",
      "[69]\ttrain-mlogloss:0.531881\ttest-mlogloss:0.571935\n",
      "[70]\ttrain-mlogloss:0.530667\ttest-mlogloss:0.571527\n",
      "[71]\ttrain-mlogloss:0.529506\ttest-mlogloss:0.571031\n",
      "[72]\ttrain-mlogloss:0.528492\ttest-mlogloss:0.570807\n",
      "[73]\ttrain-mlogloss:0.527382\ttest-mlogloss:0.570335\n",
      "[74]\ttrain-mlogloss:0.526143\ttest-mlogloss:0.569991\n",
      "[75]\ttrain-mlogloss:0.525315\ttest-mlogloss:0.569584\n",
      "[76]\ttrain-mlogloss:0.524272\ttest-mlogloss:0.56935\n",
      "[77]\ttrain-mlogloss:0.523098\ttest-mlogloss:0.568836\n",
      "[78]\ttrain-mlogloss:0.521855\ttest-mlogloss:0.568196\n",
      "[79]\ttrain-mlogloss:0.52066\ttest-mlogloss:0.56753\n",
      "[80]\ttrain-mlogloss:0.519584\ttest-mlogloss:0.56696\n",
      "[81]\ttrain-mlogloss:0.518487\ttest-mlogloss:0.566504\n",
      "[82]\ttrain-mlogloss:0.517639\ttest-mlogloss:0.56618\n",
      "[83]\ttrain-mlogloss:0.516406\ttest-mlogloss:0.565498\n",
      "[84]\ttrain-mlogloss:0.515595\ttest-mlogloss:0.565129\n",
      "[85]\ttrain-mlogloss:0.514711\ttest-mlogloss:0.56467\n",
      "[86]\ttrain-mlogloss:0.513657\ttest-mlogloss:0.564445\n",
      "[87]\ttrain-mlogloss:0.51241\ttest-mlogloss:0.564128\n",
      "[88]\ttrain-mlogloss:0.511359\ttest-mlogloss:0.563716\n",
      "[89]\ttrain-mlogloss:0.510622\ttest-mlogloss:0.563229\n",
      "[90]\ttrain-mlogloss:0.509742\ttest-mlogloss:0.562818\n",
      "[91]\ttrain-mlogloss:0.509129\ttest-mlogloss:0.562615\n",
      "[92]\ttrain-mlogloss:0.508005\ttest-mlogloss:0.562185\n",
      "[93]\ttrain-mlogloss:0.507331\ttest-mlogloss:0.562012\n",
      "[94]\ttrain-mlogloss:0.506245\ttest-mlogloss:0.561463\n",
      "[95]\ttrain-mlogloss:0.505363\ttest-mlogloss:0.561384\n",
      "[96]\ttrain-mlogloss:0.50458\ttest-mlogloss:0.561125\n",
      "[97]\ttrain-mlogloss:0.503391\ttest-mlogloss:0.560684\n",
      "[98]\ttrain-mlogloss:0.502357\ttest-mlogloss:0.560251\n",
      "[99]\ttrain-mlogloss:0.501478\ttest-mlogloss:0.559841\n",
      "[100]\ttrain-mlogloss:0.500692\ttest-mlogloss:0.55954\n",
      "[101]\ttrain-mlogloss:0.499926\ttest-mlogloss:0.559289\n",
      "[102]\ttrain-mlogloss:0.499014\ttest-mlogloss:0.559002\n",
      "[103]\ttrain-mlogloss:0.498041\ttest-mlogloss:0.558474\n",
      "[104]\ttrain-mlogloss:0.497458\ttest-mlogloss:0.558149\n",
      "[105]\ttrain-mlogloss:0.496354\ttest-mlogloss:0.557756\n",
      "[106]\ttrain-mlogloss:0.495857\ttest-mlogloss:0.557741\n",
      "[107]\ttrain-mlogloss:0.495046\ttest-mlogloss:0.557602\n",
      "[108]\ttrain-mlogloss:0.494144\ttest-mlogloss:0.5574\n",
      "[109]\ttrain-mlogloss:0.493332\ttest-mlogloss:0.557185\n",
      "[110]\ttrain-mlogloss:0.492381\ttest-mlogloss:0.556897\n",
      "[111]\ttrain-mlogloss:0.491294\ttest-mlogloss:0.556424\n",
      "[112]\ttrain-mlogloss:0.490365\ttest-mlogloss:0.55609\n",
      "[113]\ttrain-mlogloss:0.489801\ttest-mlogloss:0.555834\n",
      "[114]\ttrain-mlogloss:0.488923\ttest-mlogloss:0.55556\n",
      "[115]\ttrain-mlogloss:0.488101\ttest-mlogloss:0.555175\n",
      "[116]\ttrain-mlogloss:0.487227\ttest-mlogloss:0.555035\n",
      "[117]\ttrain-mlogloss:0.486642\ttest-mlogloss:0.554846\n",
      "[118]\ttrain-mlogloss:0.486019\ttest-mlogloss:0.554748\n",
      "[119]\ttrain-mlogloss:0.485537\ttest-mlogloss:0.554651\n",
      "[120]\ttrain-mlogloss:0.484823\ttest-mlogloss:0.554453\n",
      "[121]\ttrain-mlogloss:0.484195\ttest-mlogloss:0.554366\n",
      "[122]\ttrain-mlogloss:0.483271\ttest-mlogloss:0.554045\n",
      "[123]\ttrain-mlogloss:0.482539\ttest-mlogloss:0.55397\n",
      "[124]\ttrain-mlogloss:0.481835\ttest-mlogloss:0.553716\n",
      "[125]\ttrain-mlogloss:0.481237\ttest-mlogloss:0.553553\n",
      "[126]\ttrain-mlogloss:0.480265\ttest-mlogloss:0.553073\n",
      "[127]\ttrain-mlogloss:0.479604\ttest-mlogloss:0.552989\n",
      "[128]\ttrain-mlogloss:0.478952\ttest-mlogloss:0.552719\n",
      "[129]\ttrain-mlogloss:0.478243\ttest-mlogloss:0.552648\n",
      "[130]\ttrain-mlogloss:0.477719\ttest-mlogloss:0.552432\n",
      "[131]\ttrain-mlogloss:0.476942\ttest-mlogloss:0.552279\n",
      "[132]\ttrain-mlogloss:0.476489\ttest-mlogloss:0.552075\n",
      "[133]\ttrain-mlogloss:0.47563\ttest-mlogloss:0.551791\n",
      "[134]\ttrain-mlogloss:0.474907\ttest-mlogloss:0.551625\n",
      "[135]\ttrain-mlogloss:0.474313\ttest-mlogloss:0.551528\n",
      "[136]\ttrain-mlogloss:0.473811\ttest-mlogloss:0.551439\n",
      "[137]\ttrain-mlogloss:0.473205\ttest-mlogloss:0.551079\n",
      "[138]\ttrain-mlogloss:0.472744\ttest-mlogloss:0.550995\n",
      "[139]\ttrain-mlogloss:0.4721\ttest-mlogloss:0.550762\n",
      "[140]\ttrain-mlogloss:0.47133\ttest-mlogloss:0.550715\n",
      "[141]\ttrain-mlogloss:0.470755\ttest-mlogloss:0.550464\n",
      "[142]\ttrain-mlogloss:0.469902\ttest-mlogloss:0.550272\n",
      "[143]\ttrain-mlogloss:0.469258\ttest-mlogloss:0.550119\n",
      "[144]\ttrain-mlogloss:0.468609\ttest-mlogloss:0.549951\n",
      "[145]\ttrain-mlogloss:0.468053\ttest-mlogloss:0.549812\n",
      "[146]\ttrain-mlogloss:0.4674\ttest-mlogloss:0.549568\n",
      "[147]\ttrain-mlogloss:0.466766\ttest-mlogloss:0.549334\n",
      "[148]\ttrain-mlogloss:0.466252\ttest-mlogloss:0.549278\n",
      "[149]\ttrain-mlogloss:0.465586\ttest-mlogloss:0.549253\n",
      "[150]\ttrain-mlogloss:0.465094\ttest-mlogloss:0.549073\n",
      "[151]\ttrain-mlogloss:0.464738\ttest-mlogloss:0.549013\n",
      "[152]\ttrain-mlogloss:0.464044\ttest-mlogloss:0.548971\n",
      "[153]\ttrain-mlogloss:0.46372\ttest-mlogloss:0.548883\n",
      "[154]\ttrain-mlogloss:0.46307\ttest-mlogloss:0.548622\n",
      "[155]\ttrain-mlogloss:0.462332\ttest-mlogloss:0.54858\n",
      "[156]\ttrain-mlogloss:0.461464\ttest-mlogloss:0.548399\n",
      "[157]\ttrain-mlogloss:0.460749\ttest-mlogloss:0.548268\n",
      "[158]\ttrain-mlogloss:0.460293\ttest-mlogloss:0.548167\n",
      "[159]\ttrain-mlogloss:0.459695\ttest-mlogloss:0.548065\n",
      "[160]\ttrain-mlogloss:0.459013\ttest-mlogloss:0.547869\n",
      "[161]\ttrain-mlogloss:0.458195\ttest-mlogloss:0.547726\n",
      "[162]\ttrain-mlogloss:0.457442\ttest-mlogloss:0.54761\n",
      "[163]\ttrain-mlogloss:0.456697\ttest-mlogloss:0.547535\n",
      "[164]\ttrain-mlogloss:0.456179\ttest-mlogloss:0.547596\n",
      "[165]\ttrain-mlogloss:0.455418\ttest-mlogloss:0.547533\n",
      "[166]\ttrain-mlogloss:0.454758\ttest-mlogloss:0.547514\n",
      "[167]\ttrain-mlogloss:0.454295\ttest-mlogloss:0.547589\n",
      "[168]\ttrain-mlogloss:0.453814\ttest-mlogloss:0.547522\n",
      "[169]\ttrain-mlogloss:0.453003\ttest-mlogloss:0.547266\n",
      "[170]\ttrain-mlogloss:0.452397\ttest-mlogloss:0.547108\n",
      "[171]\ttrain-mlogloss:0.451893\ttest-mlogloss:0.547033\n",
      "[172]\ttrain-mlogloss:0.45106\ttest-mlogloss:0.5467\n",
      "[173]\ttrain-mlogloss:0.450242\ttest-mlogloss:0.546555\n",
      "[174]\ttrain-mlogloss:0.449645\ttest-mlogloss:0.546416\n",
      "[175]\ttrain-mlogloss:0.449082\ttest-mlogloss:0.546387\n",
      "[176]\ttrain-mlogloss:0.448611\ttest-mlogloss:0.546397\n",
      "[177]\ttrain-mlogloss:0.448085\ttest-mlogloss:0.546202\n",
      "[178]\ttrain-mlogloss:0.447495\ttest-mlogloss:0.546277\n",
      "[179]\ttrain-mlogloss:0.446866\ttest-mlogloss:0.546277\n",
      "[180]\ttrain-mlogloss:0.446381\ttest-mlogloss:0.546279\n",
      "[181]\ttrain-mlogloss:0.445787\ttest-mlogloss:0.54617\n",
      "[182]\ttrain-mlogloss:0.445192\ttest-mlogloss:0.546135\n",
      "[183]\ttrain-mlogloss:0.444497\ttest-mlogloss:0.545947\n",
      "[184]\ttrain-mlogloss:0.444066\ttest-mlogloss:0.54587\n",
      "[185]\ttrain-mlogloss:0.443721\ttest-mlogloss:0.545693\n",
      "[186]\ttrain-mlogloss:0.443119\ttest-mlogloss:0.545628\n",
      "[187]\ttrain-mlogloss:0.442648\ttest-mlogloss:0.545615\n",
      "[188]\ttrain-mlogloss:0.442079\ttest-mlogloss:0.545507\n",
      "[189]\ttrain-mlogloss:0.441522\ttest-mlogloss:0.545394\n",
      "[190]\ttrain-mlogloss:0.441068\ttest-mlogloss:0.545293\n",
      "[191]\ttrain-mlogloss:0.440314\ttest-mlogloss:0.545157\n",
      "[192]\ttrain-mlogloss:0.439765\ttest-mlogloss:0.545047\n",
      "[193]\ttrain-mlogloss:0.439477\ttest-mlogloss:0.545064\n",
      "[194]\ttrain-mlogloss:0.438989\ttest-mlogloss:0.544969\n",
      "[195]\ttrain-mlogloss:0.43863\ttest-mlogloss:0.544842\n",
      "[196]\ttrain-mlogloss:0.438066\ttest-mlogloss:0.544858\n",
      "[197]\ttrain-mlogloss:0.437436\ttest-mlogloss:0.544718\n",
      "[198]\ttrain-mlogloss:0.436903\ttest-mlogloss:0.544596\n",
      "[199]\ttrain-mlogloss:0.436521\ttest-mlogloss:0.544562\n",
      "[200]\ttrain-mlogloss:0.436067\ttest-mlogloss:0.54446\n",
      "[201]\ttrain-mlogloss:0.43544\ttest-mlogloss:0.54426\n",
      "[202]\ttrain-mlogloss:0.434881\ttest-mlogloss:0.544286\n",
      "[203]\ttrain-mlogloss:0.434348\ttest-mlogloss:0.544214\n",
      "[204]\ttrain-mlogloss:0.433914\ttest-mlogloss:0.544088\n",
      "[205]\ttrain-mlogloss:0.433483\ttest-mlogloss:0.544115\n",
      "[206]\ttrain-mlogloss:0.432831\ttest-mlogloss:0.544183\n",
      "[207]\ttrain-mlogloss:0.432226\ttest-mlogloss:0.544057\n",
      "[208]\ttrain-mlogloss:0.431727\ttest-mlogloss:0.543971\n",
      "[209]\ttrain-mlogloss:0.431116\ttest-mlogloss:0.543896\n",
      "[210]\ttrain-mlogloss:0.430585\ttest-mlogloss:0.543852\n",
      "[211]\ttrain-mlogloss:0.43003\ttest-mlogloss:0.543772\n",
      "[212]\ttrain-mlogloss:0.429499\ttest-mlogloss:0.54373\n",
      "[213]\ttrain-mlogloss:0.428988\ttest-mlogloss:0.543637\n",
      "[214]\ttrain-mlogloss:0.428412\ttest-mlogloss:0.543517\n",
      "[215]\ttrain-mlogloss:0.428113\ttest-mlogloss:0.543411\n",
      "[216]\ttrain-mlogloss:0.427699\ttest-mlogloss:0.543353\n",
      "[217]\ttrain-mlogloss:0.427259\ttest-mlogloss:0.543346\n",
      "[218]\ttrain-mlogloss:0.426709\ttest-mlogloss:0.543208\n",
      "[219]\ttrain-mlogloss:0.42602\ttest-mlogloss:0.543197\n",
      "[220]\ttrain-mlogloss:0.425558\ttest-mlogloss:0.543087\n",
      "[221]\ttrain-mlogloss:0.424949\ttest-mlogloss:0.54294\n",
      "[222]\ttrain-mlogloss:0.424255\ttest-mlogloss:0.542697\n",
      "[223]\ttrain-mlogloss:0.42371\ttest-mlogloss:0.542587\n",
      "[224]\ttrain-mlogloss:0.423076\ttest-mlogloss:0.542591\n",
      "[225]\ttrain-mlogloss:0.422556\ttest-mlogloss:0.542474\n",
      "[226]\ttrain-mlogloss:0.421895\ttest-mlogloss:0.542219\n",
      "[227]\ttrain-mlogloss:0.421325\ttest-mlogloss:0.542172\n",
      "[228]\ttrain-mlogloss:0.420827\ttest-mlogloss:0.542166\n",
      "[229]\ttrain-mlogloss:0.420198\ttest-mlogloss:0.541938\n",
      "[230]\ttrain-mlogloss:0.419481\ttest-mlogloss:0.541834\n",
      "[231]\ttrain-mlogloss:0.418919\ttest-mlogloss:0.541747\n",
      "[232]\ttrain-mlogloss:0.418378\ttest-mlogloss:0.541674\n",
      "[233]\ttrain-mlogloss:0.41795\ttest-mlogloss:0.541498\n",
      "[234]\ttrain-mlogloss:0.417264\ttest-mlogloss:0.541359\n",
      "[235]\ttrain-mlogloss:0.41665\ttest-mlogloss:0.541371\n",
      "[236]\ttrain-mlogloss:0.416014\ttest-mlogloss:0.541356\n",
      "[237]\ttrain-mlogloss:0.415331\ttest-mlogloss:0.541222\n",
      "[238]\ttrain-mlogloss:0.414955\ttest-mlogloss:0.541157\n",
      "[239]\ttrain-mlogloss:0.414429\ttest-mlogloss:0.540935\n",
      "[240]\ttrain-mlogloss:0.413989\ttest-mlogloss:0.540805\n",
      "[241]\ttrain-mlogloss:0.413505\ttest-mlogloss:0.540673\n",
      "[242]\ttrain-mlogloss:0.412921\ttest-mlogloss:0.540531\n",
      "[243]\ttrain-mlogloss:0.412391\ttest-mlogloss:0.540447\n",
      "[244]\ttrain-mlogloss:0.411851\ttest-mlogloss:0.540406\n",
      "[245]\ttrain-mlogloss:0.411239\ttest-mlogloss:0.540397\n",
      "[246]\ttrain-mlogloss:0.410861\ttest-mlogloss:0.540355\n",
      "[247]\ttrain-mlogloss:0.410511\ttest-mlogloss:0.54029\n",
      "[248]\ttrain-mlogloss:0.409886\ttest-mlogloss:0.5404\n",
      "[249]\ttrain-mlogloss:0.409463\ttest-mlogloss:0.540377\n",
      "[250]\ttrain-mlogloss:0.408949\ttest-mlogloss:0.54031\n",
      "[251]\ttrain-mlogloss:0.408445\ttest-mlogloss:0.540266\n",
      "[252]\ttrain-mlogloss:0.408194\ttest-mlogloss:0.540238\n",
      "[253]\ttrain-mlogloss:0.407886\ttest-mlogloss:0.540222\n",
      "[254]\ttrain-mlogloss:0.40728\ttest-mlogloss:0.540042\n",
      "[255]\ttrain-mlogloss:0.406807\ttest-mlogloss:0.540091\n",
      "[256]\ttrain-mlogloss:0.406315\ttest-mlogloss:0.540014\n",
      "[257]\ttrain-mlogloss:0.405714\ttest-mlogloss:0.539875\n",
      "[258]\ttrain-mlogloss:0.405122\ttest-mlogloss:0.539733\n",
      "[259]\ttrain-mlogloss:0.404527\ttest-mlogloss:0.539682\n",
      "[260]\ttrain-mlogloss:0.404066\ttest-mlogloss:0.539645\n",
      "[261]\ttrain-mlogloss:0.403599\ttest-mlogloss:0.539623\n",
      "[262]\ttrain-mlogloss:0.402995\ttest-mlogloss:0.539583\n",
      "[263]\ttrain-mlogloss:0.402495\ttest-mlogloss:0.539626\n",
      "[264]\ttrain-mlogloss:0.401845\ttest-mlogloss:0.539492\n",
      "[265]\ttrain-mlogloss:0.401281\ttest-mlogloss:0.539471\n",
      "[266]\ttrain-mlogloss:0.400976\ttest-mlogloss:0.539401\n",
      "[267]\ttrain-mlogloss:0.400524\ttest-mlogloss:0.539472\n",
      "[268]\ttrain-mlogloss:0.400234\ttest-mlogloss:0.539347\n",
      "[269]\ttrain-mlogloss:0.399574\ttest-mlogloss:0.539146\n",
      "[270]\ttrain-mlogloss:0.399031\ttest-mlogloss:0.53904\n",
      "[271]\ttrain-mlogloss:0.398645\ttest-mlogloss:0.53895\n",
      "[272]\ttrain-mlogloss:0.398356\ttest-mlogloss:0.538893\n",
      "[273]\ttrain-mlogloss:0.397771\ttest-mlogloss:0.538777\n",
      "[274]\ttrain-mlogloss:0.397224\ttest-mlogloss:0.538704\n",
      "[275]\ttrain-mlogloss:0.39665\ttest-mlogloss:0.53872\n",
      "[276]\ttrain-mlogloss:0.396235\ttest-mlogloss:0.538635\n",
      "[277]\ttrain-mlogloss:0.39566\ttest-mlogloss:0.538436\n",
      "[278]\ttrain-mlogloss:0.395254\ttest-mlogloss:0.538444\n",
      "[279]\ttrain-mlogloss:0.394857\ttest-mlogloss:0.538353\n",
      "[280]\ttrain-mlogloss:0.394351\ttest-mlogloss:0.538249\n",
      "[281]\ttrain-mlogloss:0.393958\ttest-mlogloss:0.538222\n",
      "[282]\ttrain-mlogloss:0.393496\ttest-mlogloss:0.538112\n",
      "[283]\ttrain-mlogloss:0.39313\ttest-mlogloss:0.538084\n",
      "[284]\ttrain-mlogloss:0.392515\ttest-mlogloss:0.538142\n",
      "[285]\ttrain-mlogloss:0.391718\ttest-mlogloss:0.53805\n",
      "[286]\ttrain-mlogloss:0.391244\ttest-mlogloss:0.537974\n",
      "[287]\ttrain-mlogloss:0.390949\ttest-mlogloss:0.537922\n",
      "[288]\ttrain-mlogloss:0.390555\ttest-mlogloss:0.537887\n",
      "[289]\ttrain-mlogloss:0.390142\ttest-mlogloss:0.537888\n",
      "[290]\ttrain-mlogloss:0.389581\ttest-mlogloss:0.53787\n",
      "[291]\ttrain-mlogloss:0.389292\ttest-mlogloss:0.537854\n",
      "[292]\ttrain-mlogloss:0.388837\ttest-mlogloss:0.537771\n",
      "[293]\ttrain-mlogloss:0.388406\ttest-mlogloss:0.53783\n",
      "[294]\ttrain-mlogloss:0.387885\ttest-mlogloss:0.537823\n",
      "[295]\ttrain-mlogloss:0.387509\ttest-mlogloss:0.537859\n",
      "[296]\ttrain-mlogloss:0.387084\ttest-mlogloss:0.537831\n",
      "[297]\ttrain-mlogloss:0.386562\ttest-mlogloss:0.53766\n",
      "[298]\ttrain-mlogloss:0.386246\ttest-mlogloss:0.537646\n",
      "[299]\ttrain-mlogloss:0.385794\ttest-mlogloss:0.537714\n",
      "[300]\ttrain-mlogloss:0.385244\ttest-mlogloss:0.537699\n",
      "[301]\ttrain-mlogloss:0.384889\ttest-mlogloss:0.537709\n",
      "[302]\ttrain-mlogloss:0.384447\ttest-mlogloss:0.53773\n",
      "[303]\ttrain-mlogloss:0.384181\ttest-mlogloss:0.537792\n",
      "[304]\ttrain-mlogloss:0.383845\ttest-mlogloss:0.537836\n",
      "[305]\ttrain-mlogloss:0.383337\ttest-mlogloss:0.537812\n",
      "[306]\ttrain-mlogloss:0.382832\ttest-mlogloss:0.537823\n",
      "[307]\ttrain-mlogloss:0.382403\ttest-mlogloss:0.537874\n",
      "[308]\ttrain-mlogloss:0.382015\ttest-mlogloss:0.537853\n",
      "[309]\ttrain-mlogloss:0.381776\ttest-mlogloss:0.537855\n",
      "[310]\ttrain-mlogloss:0.381404\ttest-mlogloss:0.537891\n",
      "[311]\ttrain-mlogloss:0.38107\ttest-mlogloss:0.537807\n",
      "[312]\ttrain-mlogloss:0.380765\ttest-mlogloss:0.537866\n",
      "[313]\ttrain-mlogloss:0.380176\ttest-mlogloss:0.537879\n",
      "[314]\ttrain-mlogloss:0.379609\ttest-mlogloss:0.537989\n",
      "[315]\ttrain-mlogloss:0.379293\ttest-mlogloss:0.537937\n",
      "[316]\ttrain-mlogloss:0.378821\ttest-mlogloss:0.538077\n",
      "[317]\ttrain-mlogloss:0.378242\ttest-mlogloss:0.538061\n",
      "[318]\ttrain-mlogloss:0.377782\ttest-mlogloss:0.538093\n",
      "Stopping. Best iteration:\n",
      "[298]\ttrain-mlogloss:0.386246\ttest-mlogloss:0.537646\n",
      "\n",
      "[0.53809261721043211]\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=7, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print(cv_scores)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(data_path+\"xgb_starter2-v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
