{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "from urllib.request import urlopen # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        site = urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "    \n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    \n",
    "        \n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    \n",
    "        \n",
    "        \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "        \n",
    "    \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "        \n",
    "        \n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "        \n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "       \n",
    "        \n",
    "    text = re.sub(\"[^a-zA-Z.+3]\",\" \", str(text))  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "        \n",
    "       \n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "        \n",
    "        \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "        \n",
    "        \n",
    "        \n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "sample = text_cleaner('http://www.indeed.com/viewjob?jk=5505e59f8e5a32a4&q=%22data+scientist%22&tk=19ftfgsmj19ti0l3&from=web&advn=1855944161169178&sjdu=QwrRXKrqZ3CNX5W-O9jEvWC1RT2wMYkGnZrqGdrncbKqQ7uwTLXzT1_ME9WQ4M-7om7mrHAlvyJT8cA_14IV5w&pub=pub-indeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.indeed.com/jobs?q=\"data+scientist\"\n"
     ]
    }
   ],
   "source": [
    "final_job = 'data+scientist' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "\n",
    "final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "print (final_site)\n",
    "base_url = 'http://www.indeed.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "html = urlopen(final_site).read()\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Jobs 1 to 10 of 3,365'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_jobs_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_numbers = re.findall('\\d+', str(num_jobs_area)) # Extract the total jobs found from the search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "    total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "else:\n",
    "    total_num_jobs = int(job_numbers[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3365"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_pages = total_num_jobs/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_descriptions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Getting page 5\n",
      "Getting page 6\n",
      "Getting page 7\n",
      "Getting page 8\n",
      "Getting page 9\n",
      "Getting page 10\n",
      "Getting page 11\n",
      "Getting page 12\n",
      "Getting page 13\n",
      "Getting page 14\n",
      "Getting page 15\n",
      "Getting page 16\n",
      "Getting page 17\n",
      "Getting page 18\n",
      "Getting page 19\n",
      "Getting page 20\n",
      "Getting page 21\n",
      "Getting page 22\n",
      "Getting page 23\n",
      "Getting page 24\n",
      "Getting page 25\n",
      "Getting page 26\n",
      "Getting page 27\n",
      "Getting page 28\n",
      "Getting page 29\n",
      "Getting page 30\n",
      "Getting page 31\n",
      "Getting page 32\n",
      "Getting page 33\n",
      "Getting page 34\n",
      "Getting page 35\n",
      "Getting page 36\n",
      "Getting page 37\n",
      "Getting page 38\n",
      "Getting page 39\n",
      "Getting page 40\n",
      "Getting page 41\n",
      "Getting page 42\n",
      "Getting page 43\n",
      "Getting page 44\n",
      "Getting page 45\n",
      "Getting page 46\n",
      "Getting page 47\n",
      "Getting page 48\n",
      "Getting page 49\n",
      "Getting page 50\n",
      "Getting page 51\n",
      "Getting page 52\n",
      "Getting page 53\n",
      "Getting page 54\n",
      "Getting page 55\n",
      "Getting page 56\n",
      "Getting page 57\n",
      "Getting page 58\n",
      "Getting page 59\n",
      "Getting page 60\n",
      "Getting page 61\n",
      "Getting page 62\n",
      "Getting page 63\n",
      "Getting page 64\n",
      "Getting page 65\n",
      "Getting page 66\n",
      "Getting page 67\n",
      "Getting page 68\n",
      "Getting page 69\n",
      "Getting page 70\n",
      "Getting page 71\n",
      "Getting page 72\n",
      "Getting page 73\n",
      "Getting page 74\n",
      "Getting page 75\n",
      "Getting page 76\n",
      "Getting page 77\n",
      "Getting page 78\n",
      "Getting page 79\n",
      "Getting page 80\n",
      "Getting page 81\n",
      "Getting page 82\n",
      "Getting page 83\n",
      "Getting page 84\n",
      "Getting page 85\n",
      "Getting page 86\n",
      "Getting page 87\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,337): # Loop through all of our search result pages\n",
    "    print ('Getting page', i)\n",
    "    start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "    current_page = ''.join([final_site, '&start=', start_num])\n",
    "    # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "        \n",
    "    html_page = urlopen(current_page).read() # Get the page\n",
    "        \n",
    "    page_obj = BeautifulSoup(html_page) # Locate all of the job links\n",
    "    job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "        \n",
    "    job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a') if link.get('href') is not None] # Get the URLS for the jobs\n",
    "        \n",
    "    job_URLS = list(filter(lambda x:'clk' in x, job_URLS)) # Now get just the job related URLS\n",
    "    for j in range(0,len(job_URLS)):\n",
    "        final_description = text_cleaner(job_URLS[j])\n",
    "        if final_description: # So that we only append when the website was accessed correctly\n",
    "            job_descriptions.append(final_description)\n",
    "            #sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "        #print ('Done with collecting the job postings!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in job_link_area.find_all('a'):\n",
    "    print (i.get('href'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
