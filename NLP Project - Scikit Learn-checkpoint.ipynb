{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "labels = {'pos':1, 'neg':0,'other':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TrainData\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\Train\\\\\"\n",
    "file=\"train.csv\"\n",
    "df=pd.DataFrame()\n",
    "Count=0\n",
    "with open(os.path.join(folder, file), 'r', encoding='ISO-8859-1') as infile:\n",
    "    reader = csv.reader(infile,skipinitialspace=True)\n",
    "    for row in reader:\n",
    "        if Count>1:\n",
    "            df = df.append([[row[0],row[1],labels[row[4]]]], ignore_index=True)\n",
    "        Count+=1\n",
    "df.columns = ['Tweet','Target', 'Stance']\n",
    "arguTrain=pd.read_csv(folder+\"ArgLex Features\\\\\"+\"ArgFeatures-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FinalDf=pd.concat([df.reset_index(drop=True), arguTrain.reset_index(drop=True)], axis=1)\n",
    "df=FinalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TestData\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\Test\\\\\"\n",
    "file=\"test.csv\"\n",
    "dfTest=pd.DataFrame()\n",
    "Count=0\n",
    "with open(os.path.join(folder, file), 'r', encoding='ISO-8859-1') as infile:\n",
    "    reader = csv.reader(infile,skipinitialspace=True)\n",
    "    for row in reader:\n",
    "        if Count>1:\n",
    "            dfTest = dfTest.append([[row[0],row[1],labels[row[4]]]], ignore_index=True)\n",
    "        Count+=1\n",
    "dfTest.columns = ['Tweet','Target', 'Stance']\n",
    "FinalDftest=pd.concat([dfTest.reset_index(drop=True), arguTrain.reset_index(drop=True)], axis=1)\n",
    "arguTrain=pd.read_csv(folder+\"ArgLex Features\\\\\"+\"ArgFeatures-test.csv\")\n",
    "dfTest=FinalDftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "      <th>assessments</th>\n",
       "      <th>authority</th>\n",
       "      <th>causation</th>\n",
       "      <th>conditionals</th>\n",
       "      <th>contrast</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>doubt</th>\n",
       "      <th>emphasis</th>\n",
       "      <th>generalization</th>\n",
       "      <th>inconsistency</th>\n",
       "      <th>inyourshoes</th>\n",
       "      <th>necessity</th>\n",
       "      <th>possibility</th>\n",
       "      <th>priority</th>\n",
       "      <th>rhetoricalquestion</th>\n",
       "      <th>structure</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @prayerbullets: I remove Nehushtan -previou...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Brainman365 @heidtjj @BenjaminLives I have so...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#God is utterly powerless without Human interv...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@David_Cameron   Miracles of #Multiculturalism...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This world needs a tight group hug. Tight enou...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet   Target  Stance  \\\n",
       "0  RT @prayerbullets: I remove Nehushtan -previou...  Atheism     2.0   \n",
       "1  @Brainman365 @heidtjj @BenjaminLives I have so...  Atheism     1.0   \n",
       "2  #God is utterly powerless without Human interv...  Atheism     0.0   \n",
       "3  @David_Cameron   Miracles of #Multiculturalism...  Atheism     0.0   \n",
       "4  This world needs a tight group hug. Tight enou...  Atheism     1.0   \n",
       "\n",
       "   assessments  authority  causation  conditionals  contrast  difficulty  \\\n",
       "0            0          0          0             0         0           0   \n",
       "1            0          0          0             0         0           0   \n",
       "2            0          0          0             0         0           0   \n",
       "3            0          0          0             0         0           0   \n",
       "4            0          0          0             0         0           0   \n",
       "\n",
       "   doubt  emphasis  generalization  inconsistency  inyourshoes  necessity  \\\n",
       "0      0         0               0              0            0          0   \n",
       "1      0         0               0              0            0          0   \n",
       "2      0         0               0              0            0          0   \n",
       "3      0         0               0              0            0          0   \n",
       "4      0         0               0              0            0          0   \n",
       "\n",
       "   possibility  priority  rhetoricalquestion  structure  wants  \n",
       "0            0         0                   0          0      0  \n",
       "1            0         0                   0          0      0  \n",
       "2            0         0                   0          0      0  \n",
       "3            0         0                   0          0      0  \n",
       "4            0         0                   0          0      0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating Separate datasets for each Target\n",
    "HillaryTrain=df.loc[df.Target==\"Hillary Clinton\"]\n",
    "AtheismTrain=df.loc[df.Target==\"Atheism\"]\n",
    "AbortionTrain=df.loc[df.Target==\"Legalization of Abortion\"]\n",
    "FeministTrain=df.loc[df.Target==\"Feminist Movement\"]\n",
    "ClimateTrain=df.loc[df.Target==\"Climate Change is a Real Concern\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating Separate datasets for each Target\n",
    "HillaryTest=dfTest.loc[dfTest.Target==\"Hillary Clinton\"]\n",
    "AtheismTest=dfTest.loc[dfTest.Target==\"Atheism\"]\n",
    "AbortionTest=dfTest.loc[dfTest.Target==\"Legalization of Abortion\"]\n",
    "FeministTest=dfTest.loc[dfTest.Target==\"Feminist Movement\"]\n",
    "ClimateTest=dfTest.loc[dfTest.Target==\"Climate Change is a Real Concern\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "      <th>assessments</th>\n",
       "      <th>authority</th>\n",
       "      <th>causation</th>\n",
       "      <th>conditionals</th>\n",
       "      <th>contrast</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>doubt</th>\n",
       "      <th>emphasis</th>\n",
       "      <th>generalization</th>\n",
       "      <th>inconsistency</th>\n",
       "      <th>inyourshoes</th>\n",
       "      <th>necessity</th>\n",
       "      <th>possibility</th>\n",
       "      <th>priority</th>\n",
       "      <th>rhetoricalquestion</th>\n",
       "      <th>structure</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @prayerbullets: I remove Nehushtan -previou...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Brainman365 @heidtjj @BenjaminLives I have so...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#God is utterly powerless without Human interv...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@David_Cameron   Miracles of #Multiculturalism...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This world needs a tight group hug. Tight enou...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet   Target  Stance  \\\n",
       "0  RT @prayerbullets: I remove Nehushtan -previou...  Atheism     2.0   \n",
       "1  @Brainman365 @heidtjj @BenjaminLives I have so...  Atheism     1.0   \n",
       "2  #God is utterly powerless without Human interv...  Atheism     0.0   \n",
       "3  @David_Cameron   Miracles of #Multiculturalism...  Atheism     0.0   \n",
       "4  This world needs a tight group hug. Tight enou...  Atheism     1.0   \n",
       "\n",
       "   assessments  authority  causation  conditionals  contrast  difficulty  \\\n",
       "0            0          0          0             0         0           0   \n",
       "1            0          0          0             0         0           0   \n",
       "2            0          0          0             0         0           0   \n",
       "3            0          0          0             0         0           0   \n",
       "4            0          0          0             0         0           0   \n",
       "\n",
       "   doubt  emphasis  generalization  inconsistency  inyourshoes  necessity  \\\n",
       "0      0         0               0              0            0          0   \n",
       "1      0         0               0              0            0          0   \n",
       "2      0         0               0              0            0          0   \n",
       "3      0         0               0              0            0          0   \n",
       "4      0         0               0              0            0          0   \n",
       "\n",
       "   possibility  priority  rhetoricalquestion  structure  wants  \n",
       "0            0         0                   0          0      0  \n",
       "1            0         0                   0          0      0  \n",
       "2            0         0                   0          0      0  \n",
       "3            0         0                   0          0      0  \n",
       "4            0         0                   0          0      0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AtheismTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming documents into feature vectors\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining and the weather is sweet'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'and': 0, 'sweet': 4, 'the': 5, 'sun': 3, 'weather': 6, 'shining': 2}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "#Assessing word relevancy via term frequency-inverse document frequency\n",
    "np.set_printoptions(precision=2)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sriram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from utils import *\n",
    "stop = stopwords.words('english')\n",
    "print (stop)\n",
    "[w for w in tokenizer('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':-)', 'Happy', ':-(', 'Sad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\":-)Happy :-( Sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training a logistic regression model for document classification\n",
    "X_train = df.ix[:,0:2]\n",
    "y_train = df['Stance']\n",
    "X_test = dfTest.ix[:,0:2]\n",
    "y_test = dfTest['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 10, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(range(3,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hillary is our best choice if we truly want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@TheView I think our country is ready for a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just gave an unhealthy amount of my hard-ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@PortiaABoulger Thank you for adding me to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hillary can not win. Here's hoping the Dems of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet\n",
       "0  Hillary is our best choice if we truly want to...\n",
       "1  @TheView I think our country is ready for a fe...\n",
       "2  I just gave an unhealthy amount of my hard-ear...\n",
       "3  @PortiaABoulger Thank you for adding me to you...\n",
       "4  Hillary can not win. Here's hoping the Dems of..."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check=df.ix[:,0:1]\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, \n",
    "                        lowercase=False, \n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0],\n",
    "              'vect__max_features':[500,750,1000,1250,1500,1600,1700]},\n",
    "             {'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0],\n",
    "              'vect__max_features':[250,500,750,1000,1250,1500,1600,1700]},\n",
    "             ]\n",
    "\n",
    "#lr_tfidf = Pipeline([('vect', tfidf),\n",
    "#                    ('clf', LogisticRegression(random_state=0, multi_class='ovr'))])\n",
    "#select_tweet.transform = select_tweet.__call__\n",
    "#select_tweet.fit = lambda x: select_tweet\n",
    "\n",
    "#lr_tfidf = Pipeline(steps=[\n",
    "#    ('select_tweet',  ColumnExtractor(cols=(0))),\n",
    "#    ('vect', tfidf),\n",
    "#    ('clf', LogisticRegression(random_state=0, multi_class='ovr'))   \n",
    "#    ])\n",
    "lr_tfidf = Pipeline(steps=[\n",
    "     ('feats',FeatureUnion([\n",
    "                ('WordVec',Pipeline([\n",
    "                ('select_tweet',  ColumnExtractor(cols=(0))),\n",
    "                ('vect', tfidf)])),\n",
    "                (\"ArguingCols\",ColumnExtractor(cols=(4)))\n",
    "            ])),\n",
    "    ('clf', LogisticRegression(random_state=0, multi_class='ovr'))   \n",
    "    ])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, \n",
    "                           scoring='accuracy',\n",
    "                           cv=5, verbose=1,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Target\n",
      "0  Hillary Clinton\n",
      "1  Hillary Clinton\n",
      "2  Hillary Clinton\n",
      "3  Hillary Clinton\n",
      "4  Hillary Clinton\n"
     ]
    }
   ],
   "source": [
    "check=X_train.ix[:,1:2]\n",
    "print(check.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...da3\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000002AEEF3D18A0, fil...lib\\site-packages\\ipykernel\\__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__pycache__\\__main__.cpython-35.pyc', '__doc__': None, '__file__': r'C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...da3\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\s...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...da3\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000002AEEF3D18A0, fil...lib\\site-packages\\ipykernel\\__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__pycache__\\__main__.cpython-35.pyc', '__doc__': None, '__file__': r'C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...da3\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\s...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    591         \n    592         If a global instance already exists, this reinitializes and starts it\n    593         \"\"\"\n    594         app = cls.instance(**kwargs)\n    595         app.initialize(argv)\n--> 596         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    597 \n    598 #-----------------------------------------------------------------------------\n    599 # utility functions, for convenience\n    600 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    437         \n    438         if self.poller is not None:\n    439             self.poller.start()\n    440         self.kernel.start()\n    441         try:\n--> 442             ioloop.IOLoop.instance().start()\n    443         except KeyboardInterrupt:\n    444             pass\n    445 \n    446 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    878                 self._events.update(event_pairs)\n    879                 while self._events:\n    880                     fd, events = self._events.popitem()\n    881                     try:\n    882                         fd_obj, handler_func = self._handlers[fd]\n--> 883                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    884                     except (OSError, IOError) as e:\n    885                         if errno_from_exception(e) == errno.EPIPE:\n    886                             # Happens when the client closes the connection\n    887                             pass\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2016-11-29T20:29:41.912963', 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'session': '80F3331E11234399845D437E373867C2', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'80F3331E11234399845D437E373867C2']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2016-11-29T20:29:41.912963', 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'session': '80F3331E11234399845D437E373867C2', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'80F3331E11234399845D437E373867C2'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2016-11-29T20:29:41.912963', 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'session': '80F3331E11234399845D437E373867C2', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'parent_header': {}})\n    386         if not silent:\n    387             self.execution_count += 1\n    388             self._publish_execute_input(code, parent, self.execution_count)\n    389 \n    390         reply_content = self.do_execute(code, silent, store_history,\n--> 391                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    392 \n    393         # Flush output before sending the reply.\n    394         sys.stdout.flush()\n    395         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='gs_lr_tfidf.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    194 \n    195         reply_content = {}\n    196         # FIXME: the shell calls the exception handler itself.\n    197         shell._reply_content = None\n    198         try:\n--> 199             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method InteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'gs_lr_tfidf.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    200         except:\n    201             status = u'error'\n    202             # FIXME: this code right now isn't being used yet by default,\n    203             # because the run_cell() call above directly fires off exception\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='gs_lr_tfidf.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2718                 self.displayhook.exec_result = result\n   2719 \n   2720                 # Execute the user code\n   2721                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2722                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2723                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2724 \n   2725                 # Reset this so later displayed values do not modify the\n   2726                 # ExecutionResult\n   2727                 self.displayhook.exec_result = None\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-55-7c8b397eb30b>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2826                     return True\n   2827 \n   2828             for i, node in enumerate(to_run_interactive):\n   2829                 mod = ast.Interactive([node])\n   2830                 code = compiler(mod, cell_name, \"single\")\n-> 2831                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000002AEF7959390, file \"<ipython-input-55-7c8b397eb30b>\", line 1>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   2832                     return True\n   2833 \n   2834             # Flush softspace\n   2835             if softspace(sys.stdout, 0):\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000002AEF7959390, file \"<ipython-input-55-7c8b397eb30b>\", line 1>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2880         outflag = 1  # happens in more places, so it's easier as default\n   2881         try:\n   2882             try:\n   2883                 self.hooks.pre_run_code_hook()\n   2884                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2885                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000002AEF7959390, file \"<ipython-input-55-7c8b397eb30b>\", line 1>\n        self.user_global_ns = {'AbortionTest':                                                 ...   0          0      0  \n\n[280 rows x 20 columns], 'AbortionTrain':                                                 ...   0          0      0  \n\n[653 rows x 20 columns], 'AtheismTest':                                                 ...   0          0      0  \n\n[219 rows x 20 columns], 'AtheismTrain':                                                 ...   0          0      0  \n\n[513 rows x 20 columns], 'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'ClimateTest':                                                 ...   0          0      0  \n\n[169 rows x 20 columns], 'ClimateTrain':                                                 ...   0          0      0  \n\n[395 rows x 20 columns], 'ColumnExtractor': <class 'utils.ColumnExtractor'>, 'Count': 1957, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, ...}\n        self.user_ns = {'AbortionTest':                                                 ...   0          0      0  \n\n[280 rows x 20 columns], 'AbortionTrain':                                                 ...   0          0      0  \n\n[653 rows x 20 columns], 'AtheismTest':                                                 ...   0          0      0  \n\n[219 rows x 20 columns], 'AtheismTrain':                                                 ...   0          0      0  \n\n[513 rows x 20 columns], 'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'ClimateTest':                                                 ...   0          0      0  \n\n[169 rows x 20 columns], 'ClimateTrain':                                                 ...   0          0      0  \n\n[395 rows x 20 columns], 'ColumnExtractor': <class 'utils.ColumnExtractor'>, 'Count': 1957, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, ...}\n   2886             finally:\n   2887                 # Reset our crash handler in place\n   2888                 sys.excepthook = old_excepthook\n   2889         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\Scripts\\<ipython-input-55-7c8b397eb30b> in <module>()\n----> 1 \n      2 \n      3 \n      4 \n      5 \n      6 gs_lr_tfidf.fit(X_train, y_train)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...jobs', refit=True, scoring='accuracy', verbose=1), X=                                                ...galization of Abortion  \n\n[2913 rows x 2 columns], y=0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64)\n    808         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    809             Target relative to X for classification or regression;\n    810             None for unsupervised learning.\n    811 \n    812         \"\"\"\n--> 813         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...obs', refit=True, scoring='accuracy', verbose=1)>\n        X =                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns]\n        y = 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64\n        self.param_grid = [{'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__max_features': [500, 750, 1000, 1250, 1500, 1600, 1700], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...]], 'vect__tokenizer': [<function tokenizer>]}, {'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__max_features': [250, 500, 750, 1000, 1250, 1500, 1600, 1700], 'vect__ngram_range': [(1, 1)], 'vect__norm': [None], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...]], 'vect__tokenizer': [<function tokenizer>], 'vect__use_idf': [False]}]\n    814 \n    815 \n    816 class RandomizedSearchCV(BaseSearchCV):\n    817     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py in _fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...jobs', refit=True, scoring='accuracy', verbose=1), X=                                                ...galization of Abortion  \n\n[2913 rows x 2 columns], y=0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    556         )(\n    557             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    558                                     train, test, self.verbose, parameters,\n    559                                     self.fit_params, return_parameters=True,\n    560                                     error_score=self.error_score)\n--> 561                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    562                 for train, test in cv)\n    563 \n    564         # Out is a list of triplet: score, estimator, n_test_samples\n    565         n_fits = len(out)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Tue Nov 29 20:29:44 2016\nPID: 4188                Python 3.5.2: C:\\Users\\sriram\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]),                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns], 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, make_scorer(accuracy_score), array([ 360,  362,  363, ..., 2910, 2911, 2912]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {}), {'error_score': 'raise', 'return_parameters': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]),                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns], 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, make_scorer(accuracy_score), array([ 360,  362,  363, ..., 2910, 2911, 2912]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), X=                                                ...galization of Abortion  \n\n[2913 rows x 2 columns], y=0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, scorer=make_scorer(accuracy_score), train=array([ 360,  362,  363, ..., 2910, 2911, 2912]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1597     fit_params = fit_params if fit_params is not None else {}\n   1598     fit_params = dict([(k, _index_param_value(X, v, train))\n   1599                       for k, v in fit_params.items()])\n   1600 \n   1601     if parameters is not None:\n-> 1602         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(st....0001,\n          verbose=0, warm_start=False))])>\n        parameters = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n   1603 \n   1604     start_time = time.time()\n   1605 \n   1606     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), **kwargs={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n    175 \n    176         Returns\n    177         -------\n    178         self\n    179         \"\"\"\n--> 180         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BasePipeline._set_params of Pipel....0001,\n          verbose=0, warm_start=False))])>\n        kwargs = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n    181         return self\n    182 \n    183     def _validate_steps(self):\n    184         names, estimators = zip(*self.steps)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), steps_attr='steps', **params={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n     64         step_names, _ = zip(*getattr(self, steps_attr))\n     65         for name in list(six.iterkeys(params)):\n     66             if '__' not in name and name in step_names:\n     67                 self._replace_step(steps_attr, name, params.pop(name))\n     68         # 3. Step parameters and other initilisation arguments\n---> 69         super(_BasePipeline, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(st....0001,\n          verbose=0, warm_start=False))])>\n        params = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n     70         return self\n     71 \n     72     def _validate_names(self, names):\n     73         if len(set(names)) != len(names):\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\base.py in set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), **params={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n    277                 name, sub_name = split\n    278                 if name not in valid_params:\n    279                     raise ValueError('Invalid parameter %s for estimator %s. '\n    280                                      'Check the list of available parameters '\n    281                                      'with `estimator.get_params().keys()`.' %\n--> 282                                      (name, self))\n        name = 'vect'\n        self = Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))])\n    283                 sub_object = valid_params[name]\n    284                 sub_object.set_params(**{sub_name: value})\n    285             else:\n    286                 # simple objects case\n\nValueError: Invalid parameter vect for estimator Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,\n       transformer_list=[('WordVec', Pipeline(steps=[('select_tweet', ColumnExtractor(cols=0)), ('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lower...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 340, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py\", line 1602, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 180, in set_params\n    self._set_params('steps', **kwargs)\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 69, in _set_params\n    super(_BasePipeline, self).set_params(**params)\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 282, in set_params\n    (name, self))\nValueError: Invalid parameter vect for estimator Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,\n       transformer_list=[('WordVec', Pipeline(steps=[('select_tweet', ColumnExtractor(cols=0)), ('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lower...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 349, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Tue Nov 29 20:29:44 2016\nPID: 4188                Python 3.5.2: C:\\Users\\sriram\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]),                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns], 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, make_scorer(accuracy_score), array([ 360,  362,  363, ..., 2910, 2911, 2912]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {}), {'error_score': 'raise', 'return_parameters': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]),                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns], 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, make_scorer(accuracy_score), array([ 360,  362,  363, ..., 2910, 2911, 2912]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), X=                                                ...galization of Abortion  \n\n[2913 rows x 2 columns], y=0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, scorer=make_scorer(accuracy_score), train=array([ 360,  362,  363, ..., 2910, 2911, 2912]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1597     fit_params = fit_params if fit_params is not None else {}\n   1598     fit_params = dict([(k, _index_param_value(X, v, train))\n   1599                       for k, v in fit_params.items()])\n   1600 \n   1601     if parameters is not None:\n-> 1602         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(st....0001,\n          verbose=0, warm_start=False))])>\n        parameters = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n   1603 \n   1604     start_time = time.time()\n   1605 \n   1606     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), **kwargs={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n    175 \n    176         Returns\n    177         -------\n    178         self\n    179         \"\"\"\n--> 180         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BasePipeline._set_params of Pipel....0001,\n          verbose=0, warm_start=False))])>\n        kwargs = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n    181         return self\n    182 \n    183     def _validate_steps(self):\n    184         names, estimators = zip(*self.steps)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), steps_attr='steps', **params={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n     64         step_names, _ = zip(*getattr(self, steps_attr))\n     65         for name in list(six.iterkeys(params)):\n     66             if '__' not in name and name in step_names:\n     67                 self._replace_step(steps_attr, name, params.pop(name))\n     68         # 3. Step parameters and other initilisation arguments\n---> 69         super(_BasePipeline, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(st....0001,\n          verbose=0, warm_start=False))])>\n        params = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n     70         return self\n     71 \n     72     def _validate_names(self, names):\n     73         if len(set(names)) != len(names):\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\base.py in set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), **params={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n    277                 name, sub_name = split\n    278                 if name not in valid_params:\n    279                     raise ValueError('Invalid parameter %s for estimator %s. '\n    280                                      'Check the list of available parameters '\n    281                                      'with `estimator.get_params().keys()`.' %\n--> 282                                      (name, self))\n        name = 'vect'\n        self = Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))])\n    283                 sub_object = valid_params[name]\n    284                 sub_object.set_params(**{sub_name: value})\n    285             else:\n    286                 # simple objects case\n\nValueError: Invalid parameter vect for estimator Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,\n       transformer_list=[('WordVec', Pipeline(steps=[('select_tweet', ColumnExtractor(cols=0)), ('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lower...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'timeout'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Tue Nov 29 20:29:44 2016\nPID: 4188                Python 3.5.2: C:\\Users\\sriram\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]),                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns], 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, make_scorer(accuracy_score), array([ 360,  362,  363, ..., 2910, 2911, 2912]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {}), {'error_score': 'raise', 'return_parameters': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]),                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns], 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, make_scorer(accuracy_score), array([ 360,  362,  363, ..., 2910, 2911, 2912]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), X=                                                ...galization of Abortion  \n\n[2913 rows x 2 columns], y=0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, scorer=make_scorer(accuracy_score), train=array([ 360,  362,  363, ..., 2910, 2911, 2912]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1597     fit_params = fit_params if fit_params is not None else {}\n   1598     fit_params = dict([(k, _index_param_value(X, v, train))\n   1599                       for k, v in fit_params.items()])\n   1600 \n   1601     if parameters is not None:\n-> 1602         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(st....0001,\n          verbose=0, warm_start=False))])>\n        parameters = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n   1603 \n   1604     start_time = time.time()\n   1605 \n   1606     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), **kwargs={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n    175 \n    176         Returns\n    177         -------\n    178         self\n    179         \"\"\"\n--> 180         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BasePipeline._set_params of Pipel....0001,\n          verbose=0, warm_start=False))])>\n        kwargs = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n    181         return self\n    182 \n    183     def _validate_steps(self):\n    184         names, estimators = zip(*self.steps)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), steps_attr='steps', **params={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n     64         step_names, _ = zip(*getattr(self, steps_attr))\n     65         for name in list(six.iterkeys(params)):\n     66             if '__' not in name and name in step_names:\n     67                 self._replace_step(steps_attr, name, params.pop(name))\n     68         # 3. Step parameters and other initilisation arguments\n---> 69         super(_BasePipeline, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(st....0001,\n          verbose=0, warm_start=False))])>\n        params = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n     70         return self\n     71 \n     72     def _validate_names(self, names):\n     73         if len(set(names)) != len(names):\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\base.py in set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), **params={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n    277                 name, sub_name = split\n    278                 if name not in valid_params:\n    279                     raise ValueError('Invalid parameter %s for estimator %s. '\n    280                                      'Check the list of available parameters '\n    281                                      'with `estimator.get_params().keys()`.' %\n--> 282                                      (name, self))\n        name = 'vect'\n        self = Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))])\n    283                 sub_object = valid_params[name]\n    284                 sub_object.set_params(**{sub_name: value})\n    285             else:\n    286                 # simple objects case\n\nValueError: Invalid parameter vect for estimator Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,\n       transformer_list=[('WordVec', Pipeline(steps=[('select_tweet', ColumnExtractor(cols=0)), ('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lower...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-7c8b397eb30b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs_lr_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         \"\"\"\n\u001b[1;32m--> 813\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    559\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 561\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m                 for train, test in cv)\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    766\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...da3\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000002AEEF3D18A0, fil...lib\\site-packages\\ipykernel\\__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__pycache__\\__main__.cpython-35.pyc', '__doc__': None, '__file__': r'C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...da3\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\s...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...da3\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000002AEEF3D18A0, fil...lib\\site-packages\\ipykernel\\__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__pycache__\\__main__.cpython-35.pyc', '__doc__': None, '__file__': r'C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...da3\\\\lib\\\\site-packages\\\\ipykernel\\\\__main__.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\s...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    591         \n    592         If a global instance already exists, this reinitializes and starts it\n    593         \"\"\"\n    594         app = cls.instance(**kwargs)\n    595         app.initialize(argv)\n--> 596         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    597 \n    598 #-----------------------------------------------------------------------------\n    599 # utility functions, for convenience\n    600 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    437         \n    438         if self.poller is not None:\n    439             self.poller.start()\n    440         self.kernel.start()\n    441         try:\n--> 442             ioloop.IOLoop.instance().start()\n    443         except KeyboardInterrupt:\n    444             pass\n    445 \n    446 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    878                 self._events.update(event_pairs)\n    879                 while self._events:\n    880                     fd, events = self._events.popitem()\n    881                     try:\n    882                         fd_obj, handler_func = self._handlers[fd]\n--> 883                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    884                     except (OSError, IOError) as e:\n    885                         if errno_from_exception(e) == errno.EPIPE:\n    886                             # Happens when the client closes the connection\n    887                             pass\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2016-11-29T20:29:41.912963', 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'session': '80F3331E11234399845D437E373867C2', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'80F3331E11234399845D437E373867C2']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2016-11-29T20:29:41.912963', 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'session': '80F3331E11234399845D437E373867C2', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'80F3331E11234399845D437E373867C2'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2016-11-29T20:29:41.912963', 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'session': '80F3331E11234399845D437E373867C2', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '4999AB768941486EA3A4140DC7D6B64F', 'msg_type': 'execute_request', 'parent_header': {}})\n    386         if not silent:\n    387             self.execution_count += 1\n    388             self._publish_execute_input(code, parent, self.execution_count)\n    389 \n    390         reply_content = self.do_execute(code, silent, store_history,\n--> 391                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    392 \n    393         # Flush output before sending the reply.\n    394         sys.stdout.flush()\n    395         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='gs_lr_tfidf.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    194 \n    195         reply_content = {}\n    196         # FIXME: the shell calls the exception handler itself.\n    197         shell._reply_content = None\n    198         try:\n--> 199             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method InteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'gs_lr_tfidf.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    200         except:\n    201             status = u'error'\n    202             # FIXME: this code right now isn't being used yet by default,\n    203             # because the run_cell() call above directly fires off exception\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='gs_lr_tfidf.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2718                 self.displayhook.exec_result = result\n   2719 \n   2720                 # Execute the user code\n   2721                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2722                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2723                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2724 \n   2725                 # Reset this so later displayed values do not modify the\n   2726                 # ExecutionResult\n   2727                 self.displayhook.exec_result = None\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-55-7c8b397eb30b>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2826                     return True\n   2827 \n   2828             for i, node in enumerate(to_run_interactive):\n   2829                 mod = ast.Interactive([node])\n   2830                 code = compiler(mod, cell_name, \"single\")\n-> 2831                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000002AEF7959390, file \"<ipython-input-55-7c8b397eb30b>\", line 1>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   2832                     return True\n   2833 \n   2834             # Flush softspace\n   2835             if softspace(sys.stdout, 0):\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000002AEF7959390, file \"<ipython-input-55-7c8b397eb30b>\", line 1>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2880         outflag = 1  # happens in more places, so it's easier as default\n   2881         try:\n   2882             try:\n   2883                 self.hooks.pre_run_code_hook()\n   2884                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2885                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000002AEF7959390, file \"<ipython-input-55-7c8b397eb30b>\", line 1>\n        self.user_global_ns = {'AbortionTest':                                                 ...   0          0      0  \n\n[280 rows x 20 columns], 'AbortionTrain':                                                 ...   0          0      0  \n\n[653 rows x 20 columns], 'AtheismTest':                                                 ...   0          0      0  \n\n[219 rows x 20 columns], 'AtheismTrain':                                                 ...   0          0      0  \n\n[513 rows x 20 columns], 'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'ClimateTest':                                                 ...   0          0      0  \n\n[169 rows x 20 columns], 'ClimateTrain':                                                 ...   0          0      0  \n\n[395 rows x 20 columns], 'ColumnExtractor': <class 'utils.ColumnExtractor'>, 'Count': 1957, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, ...}\n        self.user_ns = {'AbortionTest':                                                 ...   0          0      0  \n\n[280 rows x 20 columns], 'AbortionTrain':                                                 ...   0          0      0  \n\n[653 rows x 20 columns], 'AtheismTest':                                                 ...   0          0      0  \n\n[219 rows x 20 columns], 'AtheismTrain':                                                 ...   0          0      0  \n\n[513 rows x 20 columns], 'BaseEstimator': <class 'sklearn.base.BaseEstimator'>, 'ClimateTest':                                                 ...   0          0      0  \n\n[169 rows x 20 columns], 'ClimateTrain':                                                 ...   0          0      0  \n\n[395 rows x 20 columns], 'ColumnExtractor': <class 'utils.ColumnExtractor'>, 'Count': 1957, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, ...}\n   2886             finally:\n   2887                 # Reset our crash handler in place\n   2888                 sys.excepthook = old_excepthook\n   2889         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\Scripts\\<ipython-input-55-7c8b397eb30b> in <module>()\n----> 1 \n      2 \n      3 \n      4 \n      5 \n      6 gs_lr_tfidf.fit(X_train, y_train)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py in fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...jobs', refit=True, scoring='accuracy', verbose=1), X=                                                ...galization of Abortion  \n\n[2913 rows x 2 columns], y=0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64)\n    808         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    809             Target relative to X for classification or regression;\n    810             None for unsupervised learning.\n    811 \n    812         \"\"\"\n--> 813         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...obs', refit=True, scoring='accuracy', verbose=1)>\n        X =                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns]\n        y = 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64\n        self.param_grid = [{'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__max_features': [500, 750, 1000, 1250, 1500, 1600, 1700], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...]], 'vect__tokenizer': [<function tokenizer>]}, {'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__max_features': [250, 500, 750, 1000, 1250, 1500, 1600, 1700], 'vect__ngram_range': [(1, 1)], 'vect__norm': [None], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...]], 'vect__tokenizer': [<function tokenizer>], 'vect__use_idf': [False]}]\n    814 \n    815 \n    816 class RandomizedSearchCV(BaseSearchCV):\n    817     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py in _fit(self=GridSearchCV(cv=5, error_score='raise',\n       e...jobs', refit=True, scoring='accuracy', verbose=1), X=                                                ...galization of Abortion  \n\n[2913 rows x 2 columns], y=0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    556         )(\n    557             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    558                                     train, test, self.verbose, parameters,\n    559                                     self.fit_params, return_parameters=True,\n    560                                     error_score=self.error_score)\n--> 561                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    562                 for train, test in cv)\n    563 \n    564         # Out is a list of triplet: score, estimator, n_test_samples\n    565         n_fits = len(out)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Tue Nov 29 20:29:44 2016\nPID: 4188                Python 3.5.2: C:\\Users\\sriram\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]),                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns], 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, make_scorer(accuracy_score), array([ 360,  362,  363, ..., 2910, 2911, 2912]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {}), {'error_score': 'raise', 'return_parameters': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]),                                                 ...galization of Abortion  \n\n[2913 rows x 2 columns], 0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, make_scorer(accuracy_score), array([ 360,  362,  363, ..., 2910, 2911, 2912]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), 1, {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), X=                                                ...galization of Abortion  \n\n[2913 rows x 2 columns], y=0       1\n1       0\n2       0\n3       1\n4       ... 0\n2911    0\n2912    0\nName: Stance, dtype: int64, scorer=make_scorer(accuracy_score), train=array([ 360,  362,  363, ..., 2910, 2911, 2912]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...42, 843, 844, 845, 847, 848, 849, 850, 854, 856]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1597     fit_params = fit_params if fit_params is not None else {}\n   1598     fit_params = dict([(k, _index_param_value(X, v, train))\n   1599                       for k, v in fit_params.items()])\n   1600 \n   1601     if parameters is not None:\n-> 1602         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(st....0001,\n          verbose=0, warm_start=False))])>\n        parameters = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n   1603 \n   1604     start_time = time.time()\n   1605 \n   1606     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), **kwargs={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n    175 \n    176         Returns\n    177         -------\n    178         self\n    179         \"\"\"\n--> 180         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BasePipeline._set_params of Pipel....0001,\n          verbose=0, warm_start=False))])>\n        kwargs = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n    181         return self\n    182 \n    183     def _validate_steps(self):\n    184         names, estimators = zip(*self.steps)\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), steps_attr='steps', **params={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n     64         step_names, _ = zip(*getattr(self, steps_attr))\n     65         for name in list(six.iterkeys(params)):\n     66             if '__' not in name and name in step_names:\n     67                 self._replace_step(steps_attr, name, params.pop(name))\n     68         # 3. Step parameters and other initilisation arguments\n---> 69         super(_BasePipeline, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(st....0001,\n          verbose=0, warm_start=False))])>\n        params = {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>}\n     70         return self\n     71 \n     72     def _validate_names(self, names):\n     73         if len(set(names)) != len(names):\n\n...........................................................................\nC:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\base.py in set_params(self=Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))]), **params={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__max_features': 500, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer>})\n    277                 name, sub_name = split\n    278                 if name not in valid_params:\n    279                     raise ValueError('Invalid parameter %s for estimator %s. '\n    280                                      'Check the list of available parameters '\n    281                                      'with `estimator.get_params().keys()`.' %\n--> 282                                      (name, self))\n        name = 'vect'\n        self = Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,...0.0001,\n          verbose=0, warm_start=False))])\n    283                 sub_object = valid_params[name]\n    284                 sub_object.set_params(**{sub_name: value})\n    285             else:\n    286                 # simple objects case\n\nValueError: Invalid parameter vect for estimator Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,\n       transformer_list=[('WordVec', Pipeline(steps=[('select_tweet', ColumnExtractor(cols=0)), ('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lower...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__penalty': 'l2', 'vect__tokenizer': <function tokenizer at 0x000002AEF76C06A8>, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'clf__C': 1.0, 'vect__ngram_range': (1, 1), 'vect__max_features': 1500} \n",
      "CV Accuracy: 0.696\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.715\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06291560102301791"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for i in y_test if i==2])/dfTest.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'RT @prayerbullets: I remove Nehushtan -previous moves of God that have become idols, from the high places -2 Kings 18:4 #SemST',\n",
       "       '@Brainman365 @heidtjj @BenjaminLives I have sought the truth of my soul and found it strong enough to stand on its own merits. #SemST',\n",
       "       '#God is utterly powerless without Human intervention... #SemST',\n",
       "       ...,\n",
       "       '@JoeyBats19 Join Twitter Trump brigade #onethousandtweets to support message #MakeAmericaGreatAgain @realDonaldTrump #SemST',\n",
       "       \"Trump's outlandish statements is political strategy. No one can be that stupid. #GOP #Strategy #SemST\",\n",
       "       '@RMConservative @DagnyRed ,P.C. is out of control,stop being offended ,we are a country of free thinkers,#1A #SemST'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Matt Terry <matt.terry@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    ('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('subject', Pipeline([\n",
    "                ('selector', ItemSelector(key='subject')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=50)),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('body_bow', Pipeline([\n",
    "                ('selector', ItemSelector(key='body')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=50)),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for pulling ad hoc features from post's body\n",
    "            ('body_stats', Pipeline([\n",
    "                ('selector', ItemSelector(key='body')),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'subject': 0.8,\n",
    "            'body_bow': 0.5,\n",
    "            'body_stats': 1.0,\n",
    "        },\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('svc', SVC(kernel='linear')),\n",
    "])\n",
    "\n",
    "# limit the list of categories to make running this example faster.\n",
    "categories = ['alt.atheism', 'talk.religion.misc']\n",
    "train = fetch_20newsgroups(random_state=1,\n",
    "                           subset='train',\n",
    "                           categories=categories,\n",
    "                           )\n",
    "test = fetch_20newsgroups(random_state=1,\n",
    "                          subset='test',\n",
    "                          categories=categories,\n",
    "                          )\n",
    "\n",
    "pipeline.fit(train.data, train.target)\n",
    "y = pipeline.predict(test.data)\n",
    "print(classification_report(y, test.target))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
