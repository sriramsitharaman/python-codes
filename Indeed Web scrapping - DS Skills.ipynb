{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "from urllib.request import urlopen # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        site = urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "    \n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    \n",
    "        \n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    \n",
    "        \n",
    "        \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "        \n",
    "    \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "        \n",
    "        \n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "        \n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "       \n",
    "        \n",
    "    text = re.sub(\"[^a-zA-Z.+3]\",\" \", str(text))  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "        \n",
    "       \n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "        \n",
    "        \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "        \n",
    "        \n",
    "        \n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "sample = text_cleaner('http://www.indeed.com/viewjob?jk=5505e59f8e5a32a4&q=%22data+scientist%22&tk=19ftfgsmj19ti0l3&from=web&advn=1855944161169178&sjdu=QwrRXKrqZ3CNX5W-O9jEvWC1RT2wMYkGnZrqGdrncbKqQ7uwTLXzT1_ME9WQ4M-7om7mrHAlvyJT8cA_14IV5w&pub=pub-indeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.indeed.com/jobs?q=\"data+scientist\"\n"
     ]
    }
   ],
   "source": [
    "final_job = 'data+scientist' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "\n",
    "final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "print (final_site)\n",
    "base_url = 'http://www.indeed.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html = urlopen(final_site).read()\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Jobs 1 to 10 of 3,368'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_jobs_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_numbers = re.findall('\\d+', str(num_jobs_area)) # Extract the total jobs found from the search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "    total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "else:\n",
    "    total_num_jobs = int(job_numbers[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3368"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_pages = total_num_jobs/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_descriptions1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,50): # Loop through all of our search result pages\n",
    "    print ('Getting page', i)\n",
    "    start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "    current_page = ''.join([final_site, '&start=', start_num])\n",
    "    # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "        \n",
    "    html_page = urlopen(current_page).read() # Get the page\n",
    "        \n",
    "    page_obj = BeautifulSoup(html_page, \"lxml\") # Locate all of the job links\n",
    "    job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "        \n",
    "    job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a') if link.get('href') is not None] # Get the URLS for the jobs\n",
    "        \n",
    "    job_URLS = list(filter(lambda x:'clk' in x, job_URLS)) # Now get just the job related URLS\n",
    "    for j in range(0,len(job_URLS)):\n",
    "        final_description = text_cleaner(job_URLS[j])\n",
    "        if final_description: # So that we only append when the website was accessed correctly\n",
    "            job_descriptions1.append(final_description)\n",
    "            #sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "        #print ('Done with collecting the job postings!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder=\"C:\\\\Users\\\\Sriram\\\\Desktop\\\\\"\n",
    "text=\"\"\n",
    "for i in range(1,7):\n",
    "    f = open(folder+\"indeed-\"+str(i)+\".csv\")\n",
    "    for row in f:\n",
    "        text+=row.replace(\"\\n\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231313"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "raw = f.read()\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bgs = nltk.bigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(bgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in fdist.items():\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='C:\\\\Users\\\\Sriram\\\\Desktop\\\\indeed-1.csv' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the texts into tokens\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens = [token.lower() for token in tokens if len(token) > 1] #same as unigrams\n",
    "bi_tokens = bigrams(tokens)\n",
    "tri_tokens = trigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "unigramsDict = {}\n",
    "for token in tokens:\n",
    "  if token not in unigramsDict:\n",
    "    unigramsDict[token] = 1\n",
    "  else:\n",
    "    unigramsDict[token] += 1\n",
    "bigramsDict = {}\n",
    "for token in bi_tokens:\n",
    "  if token not in bigramsDict:\n",
    "    bigramsDict[token] = 1\n",
    "  else:\n",
    "    bigramsDict[token] += 1\n",
    "trigramsDict = {}\n",
    "for token in tri_tokens:\n",
    "  if token not in trigramsDict:\n",
    "    trigramsDict[token] = 1\n",
    "  else:\n",
    "    trigramsDict[token] += 1\n",
    "sorted_unigrams = sorted(unigramsDict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "sorted_bigrams = sorted(bigramsDict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "sorted_trigrams = sorted(trigramsDict.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for token in bi_tokens:\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniFile=open(folder+\"unigrams.csv\",\"w\")\n",
    "for word in sorted_unigrams:\n",
    "    uniFile.write(word[0]+\",\"+str(word[1])+\"\\n\")\n",
    "\n",
    "uniFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
