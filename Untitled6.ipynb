{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-42987ce0da3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import datetime\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "############################################################################\n",
    "# Function to Write result in csv file to submit \n",
    "###########################################################################\n",
    "\n",
    "def write_to_csv(output,score):\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    prediction_file_object = csv.writer(f)\n",
    "    prediction_file_object.writerow([\"Id\",\"SalePrice\"])  # don't forget the headers\n",
    "\n",
    "    for i in range(len(test)):\n",
    "        prediction_file_object.writerow([test[\"Id\"][test.index[i]], (output[i])])\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# Function to process features \n",
    "###########################################################################\n",
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values) # list train features\n",
    "    testval = list(test.columns.values) # list test features\n",
    "    output = list(set(trainval) & set(testval)) # check wich features are in common (remove the outcome column)\n",
    "    output.remove('Id') # remove non-usefull id column\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_features(train,test):\n",
    "    tables=[test,train]\n",
    "    print (\"Handling missing values...\")\n",
    "    total_missing=train.isnull().sum()\n",
    "    to_delete=total_missing[total_missing>(1460/3.)] # select features with more than 1/3 missing values\n",
    "    for table in tables:\n",
    "        table.drop(to_delete.index.tolist(),axis=1, inplace=True)\n",
    "            \n",
    "    print (\"Filling Nan...\")\n",
    "    numerical_features=test.select_dtypes(include=[\"float\",\"int\",\"bool\"]).columns.values\n",
    "    categorical_features=train.select_dtypes(include=[\"object\"]).columns.values\n",
    "    for table in tables: \n",
    "        for feature in numerical_features: \n",
    "            table[feature].fillna(train[feature].median(), inplace = True) # replace by median value\n",
    "        for feature in categorical_features: \n",
    "            table[feature].fillna(train[feature].value_counts().idxmax(), inplace = True) # replace by most frequent value\n",
    "\n",
    "    print (\"Handling categorical features...\")\n",
    "    for feature in categorical_features: # Encode categorical features\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(train[feature])\n",
    "        for table in tables: \n",
    "            table[feature]=le.transform(table[feature])\n",
    "    \n",
    "    print (\"Getting features...\")\n",
    "    features = get_features(train,test)\n",
    "    \n",
    "    return train,test,features\n",
    "\n",
    "def train_and_test_linear(train,test,features,target='SalePrice'): # simple xgboost\n",
    "    subsample = 0.8\n",
    "    colsample_bytree = 0.8\n",
    "    num_boost_round = 1000 #115 originally \n",
    "    early_stopping_rounds = 50\n",
    "    test_size = 0.2 # 0.1 originally\n",
    "    \n",
    "    start_time = time.time()\n",
    "   \n",
    "    # start the training\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:linear\",\n",
    "        \"booster\" : \"gblinear\", #\"gbtree\",# default\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"subsample\": subsample, # collect 80% of the data only to prevent overfitting\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "\n",
    "    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=0) # randomly split into 90% test and 10% CV -> still has the outcome at this point\n",
    "    y_train = np.log(X_train[target]) # define y as the outcome column, apply log to have same error as the leaderboard\n",
    "    y_valid = np.log(X_valid[target])\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train) # DMatrix are matrix for xgboost\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')] # list of things to evaluate and print\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True) # find the best score\n",
    "    score = gbm.best_score #roc_auc_score(X_valid[target].values, check)\n",
    "    print('Last error value: {:.6f}'.format(score))\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]))\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "    return test_prediction, score    \n",
    "\n",
    "def train_and_test_tree(train,test,features,target='SalePrice'): # simple xgboost\n",
    "    eta_list = [0.1,0.2] # list of parameters to try\n",
    "    max_depth_list = [4,6,8] # list of parameters to try\n",
    "    subsample = 0.8\n",
    "    colsample_bytree = 0.8\n",
    "    \n",
    "    num_boost_round = 400 \n",
    "    early_stopping_rounds = 10\n",
    "    test_size = 0.2 \n",
    "    \n",
    "    start_time = time.time()\n",
    "   \n",
    "    # start the training\n",
    "    array_score=np.ndarray((len(eta_list)*len(max_depth_list),3)) # store score values\n",
    "    i=0\n",
    "    for eta,max_depth in list(itertools.product(eta_list, max_depth_list)): # Loop over parameters to find the better set\n",
    "        print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "        params = {\n",
    "            \"objective\": \"reg:linear\",\n",
    "            \"booster\" : \"gbtree\", \n",
    "            \"eval_metric\": \"rmse\", # this is the metric for the leardboard\n",
    "            \"eta\": eta, # shrinking parameters to prevent overfitting\n",
    "            \"tree_method\": 'exact',\n",
    "            \"max_depth\": max_depth,\n",
    "            \"subsample\": subsample, # collect 80% of the data only to prevent overfitting\n",
    "            \"colsample_bytree\": colsample_bytree,\n",
    "            \"silent\": 1,\n",
    "            \"seed\": 0,\n",
    "        }\n",
    "    \n",
    "        X_train, X_valid = train_test_split(train, test_size=test_size, random_state=0) # randomly split into 90% test and 10% CV -> still has the outcome at this point\n",
    "        y_train = np.log(X_train[target]) # define y as the outcome column\n",
    "        y_valid = np.log(X_valid[target])\n",
    "        dtrain = xgb.DMatrix(X_train[features], y_train) # DMatrix are matrix for xgboost\n",
    "        dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')] # list of things to evaluate and print\n",
    "        gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True) # find the best score\n",
    "\n",
    "        print(\"Validating...\")\n",
    "        score = gbm.best_score \n",
    "        print('Last error value: {:.6f}'.format(score))\n",
    "        array_score[i][0]=eta\n",
    "        array_score[i][1]=max_depth\n",
    "        array_score[i][2]=score\n",
    "        i+=1\n",
    "    df_score=pd.DataFrame(array_score,columns=['eta','max_depth','SalePrice'])\n",
    "    print(\"df_score : \\n\", df_score)\n",
    "    #create_feature_map(features)\n",
    "    importance = gbm.get_fscore()\n",
    "    importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "    print('Importance array: ', importance)\n",
    "    np.save(\"features_importance\",importance) # save feature importance for latter use\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_ntree_limit) # only predict with the last set of parameters\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "    return test_prediction, score \n",
    "\n",
    "def train_and_test_Kfold(train,test,features,target='SalePrice'): # add Kfold\n",
    "    eta_list = [0.02] # list of parameters to try\n",
    "    max_depth_list = [6]\n",
    "    subsample = 1 # No subsampling, as we already use Kfold latter and we don't have that much data\n",
    "    colsample_bytree = 1\n",
    "    \n",
    "    num_boost_round = 5000 # for small eta, increase this one\n",
    "    early_stopping_rounds = 500\n",
    "    n_folds=3 \n",
    "    start_time = time.time()\n",
    "   \n",
    "\n",
    "    # start the training\n",
    "    array_score=np.ndarray((len(eta_list)*len(max_depth_list),4)) # store score values\n",
    "    i=0\n",
    "    for eta,max_depth in list(itertools.product(eta_list, max_depth_list)): # Loop over parameters to find the better set\n",
    "        print('XGBoost params. ETA: {}, MAX_DEPTH: {}'.format(eta, max_depth))\n",
    "        params = {\n",
    "            \"objective\": \"reg:linear\",\n",
    "            \"booster\" : \"gbtree\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": eta, # shrinking parameters to prevent overfitting\n",
    "            \"tree_method\": 'exact',\n",
    "            \"max_depth\": max_depth,\n",
    "            \"subsample\": subsample, # collect 80% of the data only to prevent overfitting\n",
    "            \"colsample_bytree\": colsample_bytree,\n",
    "            \"silent\": 1,\n",
    "            \"seed\": 0,\n",
    "        }\n",
    "        kf = KFold(len(train), n_folds=n_folds)\n",
    "        test_prediction=np.ndarray((n_folds,len(test)))\n",
    "        fold=0\n",
    "        fold_score=[]\n",
    "        for train_index, cv_index in kf:\n",
    "            X_train, X_valid    = train[features].as_matrix()[train_index], train[features].as_matrix()[cv_index]\n",
    "            y_train, y_valid    = np.log(train[target].as_matrix()[train_index]), np.log(train[target].as_matrix()[cv_index])\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_train, y_train) # DMatrix are matrix for xgboost\n",
    "            dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'eval')] # list of things to evaluate and print\n",
    "            gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True) # find the best score\n",
    "\n",
    "            print(\"Validating...\")\n",
    "            check = gbm.predict(xgb.DMatrix(X_valid)) # get the best score\n",
    "            score = gbm.best_score\n",
    "            print('Check last score value: {:.6f}'.format(score))\n",
    "            fold_score.append(score)\n",
    "            importance = gbm.get_fscore()\n",
    "            importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "            print('Importance array for fold {} :\\n {}'.format(fold, importance))\n",
    "            #np.save(\"features_importance\",importance)\n",
    "            print(\"Predict test set...\")\n",
    "            prediction=gbm.predict(xgb.DMatrix(test[features].as_matrix()))\n",
    "            #np.save(\"prediction_eta%s_depth%s_fold%s\" %(eta,max_depth,fold),prediction) # You can save all the folds prediction to check for errors in code\n",
    "            test_prediction[fold]=prediction\n",
    "            fold = fold + 1\n",
    "        mean_score=np.mean(fold_score)\n",
    "        print(\"Mean Score : {}, eta : {}, depth : {}\\n\".format(mean_score,eta,max_depth))\n",
    "        array_score[i][0]=eta\n",
    "        array_score[i][1]=max_depth\n",
    "        array_score[i][2]=mean_score\n",
    "        array_score[i][3]=np.std(fold_score)\n",
    "        i+=1\n",
    "    final_prediction=test_prediction.mean(axis=0)\n",
    "    df_score=pd.DataFrame(array_score,columns=['eta','max_depth','mean_score','std_score'])\n",
    "    print (\"df_score : \\n\", df_score)# get the complete array of scores to choose the right parameters\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "    return final_prediction, mean_score \n",
    "\n",
    "\n",
    "############################################################################\n",
    "# Main code\n",
    "###########################################################################\n",
    "\n",
    "num_features = None # Choose how many features you want to use. None = all\n",
    "\n",
    "train = pd.read_csv(\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Kaggle\\\\Home Price\\\\train.csv\") # read train data\n",
    "test = pd.read_csv(\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Kaggle\\\\Home Price\\\\test.csv\") # read test data\n",
    "\n",
    "train,test,features = process_features(train,test)\n",
    "\n",
    "#test_prediction,score = train_and_test_linear(train,test,features)\n",
    "#test_prediction,score = train_and_test_tree(train,test,features) # run at least once this one to get the features importance\n",
    "#features=np.load(\"features_importance.npy\")\n",
    "test_prediction,score = train_and_test_Kfold(train,test,features[:num_features]) \n",
    "\n",
    "write_to_csv(np.exp(test_prediction),score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
