{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "path='C:\\\\Users\\\\Shahidhya\\\\OneDrive\\\\I524 Project\\\\3. Twitter Streaming\\\\set 1\\\\'\n",
    "tweetFile='Twitter_data2.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sriram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from time import strptime\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import zipcode\n",
    "import sys, errno\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "runCount=0\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"141817420-ViMO9ic2MuVmjw4u04CACINnCA0MIJEs2uaPbkYX\"\n",
    "access_token_secret = \"LWNQKJYkHJnrAjNsH7LnrkKWmnf5qZ9akizidiWjbLhOy\"\n",
    "consumer_key = \"SyLwuJP6pzy4FevmLMOmcWdpf\"\n",
    "consumer_secret = \"XfZkuRRj5yqVmReRkAVvVFm9t6vaPHVeoXEdg85Iuqb8k524pU\"\n",
    "\n",
    "tweets_data = []\n",
    "\n",
    "stop = stopwords.words('english') + ['and']\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "Count=0\n",
    "stop = stopwords.words('english')\n",
    "#This is a basic listener that just prints received tweets to stdout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze data started\n",
      "                                                text  \\\n",
      "0  I used to be a morning person but things chang...   \n",
      "1              RT @taxusanemone: I'm lonely and blue   \n",
      "2  RT @BigRadMachine: Things I like:\\n1. Cold bee...   \n",
      "3  RT @looks_killllll: white girls please stop sa...   \n",
      "4  RT @wes415: Feeling unmotivated for your next ...   \n",
      "\n",
      "                       created_at location state sentiment sentiment_cat  \\\n",
      "0  Mon May 01 00:41:29 +0000 2017     None    NA         0           Neu   \n",
      "1  Mon May 01 00:41:29 +0000 2017     None    NA     -0.05           Neg   \n",
      "2  Mon May 01 00:41:29 +0000 2017     None    NA         0           Neu   \n",
      "3  Mon May 01 00:41:29 +0000 2017     None    NA         0           Neu   \n",
      "4  Mon May 01 00:41:29 +0000 2017  Ireland    NA         0           Neu   \n",
      "\n",
      "  country_code hour  \n",
      "0                00  \n",
      "1                00  \n",
      "2                00  \n",
      "3                00  \n",
      "4                00  \n",
      "        State   value\n",
      "0     Alabama   827.0\n",
      "1      Alaska   613.0\n",
      "2     Arizona  1127.0\n",
      "3    Arkansas   188.0\n",
      "4  California  6958.0\n",
      "['Neg' 'Neu' 'Pos' 'State']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country_code' 'hour' 'sentiment' 'country' 'continent']\n",
      "Analyze data Completed in  139.42586159706116\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "13",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        global Count,tweets_data\n",
    "        Count+=1\n",
    "        #TweetCount+=1\n",
    "        if Count%4000==0:\n",
    "            print (\"Analyze data started\")\n",
    "            x=time.time()\n",
    "            analyze(tweets_data)\n",
    "            print (\"Analyze data Completed in \", time.time()-x)\n",
    "            sys.exit(errno.EACCES)\n",
    "            tweets_data=[]\n",
    "            Count=0\n",
    "        tweet = json.loads(data)\n",
    "        tweets_data.append(tweet)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    stream.filter(languages=[\"en\"],track=['a', 'e', 'i','o','u','#'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataframe(tweets_data):\n",
    "    tweets = pd.DataFrame(index=range(len(tweets_data)), \n",
    "    columns=['text','created_at','location','state','sentiment','sentiment_cat','country_code','hour'])\n",
    "    \n",
    "    for i in range(len(tweets_data)):\n",
    "        try:\n",
    "            tweets['text'][i] = tweets_data[i]['text']\n",
    "        except:\n",
    "            tweets['text'][i] = \"\"\n",
    "        try:\n",
    "            tweets['location'][i]=tweets_data[i]['user']['location']\n",
    "        except:\n",
    "            tweets['location'][i]='NA'\n",
    "        try:\n",
    "            tweets['country_code'][i]=tweets_data[i]['place']['country_code']\n",
    "        except:\n",
    "            tweets['country_code'][i]=''\n",
    "        try:\n",
    "            lon=tweets_data[i]['place']['bounding_box']['coordinates'][0][0][0]\n",
    "        except:\n",
    "            lon='NA'\n",
    "        try:\n",
    "            lat=tweets_data[i]['place']['bounding_box']['coordinates'][0][0][1]\n",
    "        except:\n",
    "            lat='NA'\n",
    "        #print (lat,lon)\n",
    "        try:\n",
    "            tweets['created_at'][i]=tweets_data[i]['created_at']\n",
    "        except:\n",
    "            tweets['created_at'][i]='NA'\n",
    "        try:\n",
    "            tweets['hour'][i]=tweets['created_at'][i][11:13]\n",
    "        except:\n",
    "            tweets['hour'][i]='NA'\n",
    "        try:\n",
    "            stateFromData=tweets['location'][i].split(',')[1]\n",
    "        except:\n",
    "            stateFromData=''\n",
    "        if len(stateFromData)==2:\n",
    "            tweets['state'][i]=stateFromData\n",
    "        else:\n",
    "            if lat!='NA':\n",
    "                radius=10\n",
    "                incre=10\n",
    "                zips=zipcode.isinradius((lat,lon),radius)\n",
    "                while len(zips)==0:\n",
    "                    radius=radius+incre\n",
    "                    zips=zipcode.isinradius((lat,lon),radius)\n",
    "                    incre=incre+10\n",
    "                myzip = zipcode.isequal(str(zips[0].zip))\n",
    "                tweets['state'][i]=myzip.state\n",
    "            else:\n",
    "                tweets['state'][i]='NA'\n",
    "        blob = TextBlob(tweets['text'][i])\n",
    "        try:\n",
    "            sentence=blob.sentences[0]\n",
    "            tweets['sentiment'][i]=float(sentence.sentiment.polarity)\n",
    "        except:\n",
    "            tweets['sentiment'][i]=0     \n",
    "        if tweets['sentiment'][i] < 0:\n",
    "            tweets['sentiment_cat'][i] = 'Neg'\n",
    "        else:\n",
    "            if tweets['sentiment'][i] > 0:\n",
    "                tweets['sentiment_cat'][i] = 'Pos'\n",
    "            else:\n",
    "                tweets['sentiment_cat'][i] = 'Neu'\n",
    "    print (tweets.head())\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze(tweets_data):\n",
    "    oldFolder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Spring 2017\\\\Big Data\\\\I524 Project\\\\Visualization\\\\Data\\\\\"\n",
    "    outputFolder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Spring 2017\\\\Big Data\\\\I524 Project\\\\Visualization\\\\OutputJS\\\\\"\n",
    "    newFolder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Spring 2017\\\\Big Data\\\\I524 Project\\\\Visualization\\\\NewData\\\\\"\n",
    "\n",
    "    #Dataframe is created from the list of json tweets, sentiment is also calculated\n",
    "    tweets=create_dataframe(tweets_data)\n",
    "    statedata=pd.read_csv(\"C:\\\\Users\\\\sriram\\\\Anaconda3\\\\Scripts\\\\states.csv\")\n",
    "    tweetsFinal=pd.merge(tweets, statedata, how='left',left_on=\"state\",right_on=\"Abbreviation\")\n",
    "\n",
    "    #UsStatewise Tweets\n",
    "    usStateOld=pd.read_csv(oldFolder+\"usStatesCount.csv\")\n",
    "    usState=pd.DataFrame({'value' : tweetsFinal.groupby( [ \"State\"] ).size()}).reset_index()\n",
    "    usState_new = pd.merge(usStateOld, usState,  how='left', left_on='State', right_on = 'State')\n",
    "    usState_new=usState_new.fillna(0)\n",
    "    usState_new['value']=usState_new['value_x']+usState_new['value_y']\n",
    "    del usState_new['value_x']\n",
    "    del usState_new['value_y']\n",
    "    usState_new.to_csv(newFolder+\"usStatesCount.csv\",index=False)\n",
    "    print (usState_new.head())\n",
    "    usStateJson=usState_new.to_json(orient = \"records\")\n",
    "    usStateJsonfinalOutput=usStateJson[33:len(usStateJson)-1].upper().replace(\"\\\"STATE\\\"\",\"ucName\").replace(\"\\\"VALUE\\\"\",\"value\")\n",
    "    with open(outputFolder+'usStates-tweetCount.json', 'w') as outfile:\n",
    "        outfile.write(usStateJsonfinalOutput)\n",
    "    \n",
    "    \n",
    "    #UsStatewise Sentiment\n",
    "    usStateSentiOld=pd.read_csv(oldFolder+\"usStates-SentiCount.csv\")\n",
    "    statesentiout=state_senti(newFolder,usStateSentiOld,tweetsFinal)\n",
    "    with open(outputFolder+'usStates-SenitCount.js', 'w') as outfile:\n",
    "        outfile.write(statesentiout)\n",
    "    \n",
    "    #TimeSeries Chart\n",
    "    timeOld=pd.read_csv(oldFolder+\"timeseries.csv\")\n",
    "    timedata=create_timechart(newFolder,timeOld,tweets)\n",
    "    with open(outputFolder+'tweet_cnt-1.js', 'w') as outfile:\n",
    "        outfile.write(timedata)\n",
    "\n",
    "    \n",
    "    #Co-occur Chart\n",
    "    nodes1,links=co_occur(tweets)\n",
    "    with open(outputFolder+'cooccur_word-1.json', 'w') as outfile:\n",
    "        outfile.write(\"{\\n\"+nodes1+\",\\n\"+links+\"}\\n\")\n",
    "        \n",
    "    #Heat World Grid\n",
    "    worldOld=pd.read_csv(oldFolder+\"Continent-hour-senti.csv\")\n",
    "    heatjson=heatworldgrid(newFolder,worldOld,tweets)\n",
    "    with open(outputFolder+'heatchart_data-1.js', 'w') as outfile:\n",
    "        outfile.write(heatjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def state_senti(newFolder,usStateSentiOld,tweetsFinal):\n",
    "\n",
    "    output2=pd.DataFrame({'value' : tweetsFinal.groupby( [ \"State\",\"sentiment_cat\"] ).size()}).reset_index()\n",
    "\n",
    "    outData=pd.pivot_table(output2,values='value', index=['State'], columns=['sentiment_cat'], aggfunc=np.sum)\n",
    "    outData=outData.fillna(0)\n",
    "    outData['State']=outData.index\n",
    "    #outData.reset_index()\n",
    "    print (outData.columns.values)\n",
    "    outData = pd.merge(usStateSentiOld, outData,  how='left', left_on='State', right_on = 'State')\n",
    "    outData=outData.fillna(0)\n",
    "    outData['Pos']=outData['Pos_x']+outData['Pos_y']\n",
    "    del outData['Pos_x']\n",
    "    del outData['Pos_y']\n",
    "    outData['Neg']=outData['Neg_x']+outData['Neg_y']\n",
    "    del outData['Neg_x']\n",
    "    del outData['Neg_y']\n",
    "    outData['Neu']=outData['Neu_x']+outData['Neu_y']\n",
    "    del outData['Neu_x']\n",
    "    del outData['Neu_y']\n",
    "    outData.to_csv(newFolder+\"usStates-SentiCount.csv\",index=False)\n",
    "    #-------------------------------------------\n",
    "    try:\n",
    "        outData['sum']=outData[['Neg', 'Neu', 'Pos']].sum(axis=1)\n",
    "        outData['max']=outData['maxFinal']=outData[['Neg', 'Neu', 'Pos']].idxmax(axis=1)\n",
    "    except:\n",
    "        outData['sum']=outData[['Neu', 'Pos']].sum(axis=1)\n",
    "        outData['max']=outData['maxFinal']=outData[[ 'Neu', 'Pos']].idxmax(axis=1)\n",
    "    #-------------------------------------------\n",
    "    for i in range(len(outData)):\n",
    "        if outData['max'][i] ==\"Pos\":\n",
    "            outData['maxFinal'][i] = '1'\n",
    "        else:\n",
    "            if outData['max'][i] ==\"Neu\":\n",
    "                outData['maxFinal'][i] = '-1'\n",
    "            else:\n",
    "                outData['maxFinal'][i] = '2'\n",
    "    \n",
    "    del outData['max']\n",
    "\n",
    "    d=\"var data =[\\n\"\n",
    "    for i in range(len(outData)):\n",
    "        row=outData.ix[i]\n",
    "        #print (row)\n",
    "        d += \"[\\'\"+row['State']+\"\\',\"+\",\".join([str(i) for i in row[:5]])+\"],\\n\"\n",
    "    \n",
    "    return d+']'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_timechart(newFolder,oldtimedata,tweets):\n",
    "    td1 = pd.DataFrame({'value' : tweets.groupby( [ \"created_at\"] ).size()}).reset_index()\n",
    "    td1['created_at'] = td1['created_at'].astype('str')\n",
    "    mask = (td1['created_at'].str.len() > 2)\n",
    "    td1=td1.loc[mask]\n",
    "    timedata = td1[td1.created_at != 'NA']\n",
    "    timedata=oldtimedata.append(timedata, ignore_index=True)\n",
    "    timedata.to_csv(newFolder+\"timeseries.csv\",index=False)\n",
    "    data1 ={}\n",
    "    data = [\"var data=[\"]\n",
    "    for i in range(0,len(timedata)):\n",
    "    \n",
    "        year = timedata['created_at'][i][-4:]\n",
    "        if (timedata['created_at'][i][4:7] == 'Jan'):\n",
    "            mon = '1'\n",
    "        else:\n",
    "            if (timedata['created_at'][i][4:7] == 'Feb'):\n",
    "                mon = '2'\n",
    "            else:\n",
    "                if (timedata['created_at'][i][4:7] == 'Mar'):\n",
    "                    mon = '3'\n",
    "                else:\n",
    "                    if (timedata['created_at'][i][4:7] == 'Apr'):\n",
    "                        mon = '4'\n",
    "                    else:\n",
    "                        if (timedata['created_at'][i][4:7] == 'May'):\n",
    "                            mon = '5'\n",
    "                        else:\n",
    "                            if (timedata['created_at'][i][4:7] == 'Jun'):\n",
    "                                mon = '6'\n",
    "                            else:\n",
    "                                if (timedata['created_at'][i][4:7] == 'Jul'):\n",
    "                                    mon = '7'\n",
    "                                else:\n",
    "                                    if (timedata['created_at'][i][4:7] == 'Aug'):\n",
    "                                        mon = '8'\n",
    "                                    else:\n",
    "                                        if (timedata['created_at'][i][4:7] == 'Sep'):\n",
    "                                            mon = '9'\n",
    "                                        else:\n",
    "                                            if (timedata['created_at'][i][4:7] == 'Oct'):\n",
    "                                                mon = '10'\n",
    "                                            else:\n",
    "                                                if (timedata['created_at'][i][4:7] == 'Nov'):\n",
    "                                                    mon = '11'\n",
    "                                                else:\n",
    "                                                    mon = '12'\n",
    "        date = timedata['created_at'][i][7:10]\n",
    "        hour = timedata['created_at'][i][10:13]\n",
    "        minu = timedata['created_at'][i][14:16]\n",
    "        sec = timedata['created_at'][i][17:20]\n",
    "        value = timedata['value'][i]\n",
    "        data1 = (\"[Date.UTC(\"+str(year)+\",\"+str(mon)+\",\"+str(date)+\",\"+str(hour)+\",\"+str(minu)+\",\"+str(sec)+\"),\"+str(value)+\"]\")\n",
    "        if (len(timedata)):\n",
    "            data.append\n",
    "        data.append(data1)\n",
    "    data = \",\\n\".join(data)+\"\\n]\"\n",
    "    data = data.replace(\"[,\",\"[\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    tokens=tokens_re.findall(s)\n",
    "    return [ x for x in tokens if 'http' not in x and len(x)>1 and x.lower() not in stop]\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def collect_pairs(lines):\n",
    "    pair_counter = Counter()\n",
    "    for line in lines:\n",
    "        unique_tokens = sorted(set(line))  # exclude duplicates in same line and sort to ensure one word is always before other\n",
    "        combos = combinations(unique_tokens, 2)\n",
    "        pair_counter += Counter(combos)\n",
    "    return pair_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Co-occurrence:\n",
    "def co_occur(tweets):\n",
    "    t2 = []\n",
    "    t1 =tweets['text']\n",
    "    for t in range(len(t1)):\n",
    "        t2.append(preprocess(t1[t]))             \n",
    "    pairs = collect_pairs(t2)\n",
    "    top_pairs = pairs.most_common(200)\n",
    "    nodes={}\n",
    "    links=[\"\\\"links\\\":[\"]\n",
    "    count =0\n",
    "    len_top=len(top_pairs)\n",
    "    nptp = np.array(top_pairs)\n",
    "    maxtp = np.max(nptp[:,1])\n",
    "    for p in range(len(top_pairs)):\n",
    "        for i in range(2):\n",
    "            if top_pairs[p][0][i] not in nodes:\n",
    "                nodes[top_pairs[p][0][i]] = count\n",
    "                count+=1\n",
    "        link=\"{ \\\"source\\\":\"+str(nodes[top_pairs[p][0][0]])+\",\\\"target\\\":\"+str(nodes[top_pairs[p][0][1]])+\",\\\"value\\\":\"+str(round(top_pairs[p][1]*10/maxtp))+\"}\"\n",
    "        links.append(link)\n",
    "    links=\",\\n\".join(links)+\"\\n]\"\n",
    "    links=links.replace(\"[,\",\"[\")\n",
    "    nodes = sorted(nodes.items(), key=lambda x: x[1])\n",
    "    nodes1=[\"\\\"nodes\\\":[\"]\n",
    "    for p in range(len(nodes)):\n",
    "        nodes1.append(\"{ \\\"name\\\":\\\"\"+nodes[p][0]+\"\\\",\\\"group\\\":\"+\"0}\")\n",
    "    nodes1=\",\\n\".join(nodes1)+\"\\n]\"\n",
    "    nodes1=nodes1.replace(\"[,\",\"[\")\n",
    "    \n",
    "    return nodes1,links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def heatworldgrid(newFolder,worldOld,tweets):\n",
    "    contdata=pd.read_csv(\"continents.txt\")\n",
    "    contdat=contdata.fillna(\"NA\")\n",
    "    tweets['sentiment']=tweets['sentiment'].apply(pd.to_numeric)\n",
    "    #print (tweets.dtypes)\n",
    "    \n",
    "    Countryhour=pd.DataFrame({'sentiment' : tweets.groupby( [\"country_code\",\"hour\"] )['sentiment'].mean()}).reset_index()\n",
    "    final=pd.merge(Countryhour, contdata, how='left',left_on=\"country_code\",right_on=\"country\")\n",
    "    print (final.columns.values)\n",
    "    del final['country']\n",
    "    #del final['Unnamed: 0']\n",
    "    del final['country_code']\n",
    "    \n",
    "    Conthour=pd.DataFrame({'sentiment' : final.groupby( [\"continent\",\"hour\"] )['sentiment'].mean()}).reset_index()\n",
    "    Conthour = pd.merge(worldOld, Conthour,  how='left', left_on=[\"continent\",\"hour\"] , right_on = [\"continent\",\"hour\"] )\n",
    "    Conthour=Conthour.fillna(0)\n",
    "    Conthour['sentiment']=(Conthour['sentiment_x']*1000000+Conthour['sentiment_y']*10000)/(1010000)\n",
    "\n",
    "    del Conthour['sentiment_x']\n",
    "    del Conthour['sentiment_y']\n",
    "    Conthour.to_csv(newFolder+\"Continent-hour-senti.csv\",index=False)\n",
    "    minVal=min(Conthour['sentiment'])\n",
    "    maxVal=max(Conthour['sentiment'])\n",
    "    outputStr=\"\"\n",
    "    uniqueCont= list(np.unique(Conthour['continent']))\n",
    "    outputStr+=\"var continent =[\"+\",\".join([\"'\"+i+\"'\" for i in uniqueCont])+\"];\\n\"\n",
    "    numCont=len(uniqueCont)\n",
    "    numHour=24\n",
    "    \n",
    "    outputStr+=\"var hour =[\"+\",\".join([\"'\"+str(i)+\"'\" for i in range(numHour)])+\"];\\n\"\n",
    "    outMatrix=np.zeros(shape=(numCont,numHour))\n",
    "    outputStr+=\"var data=[\"\n",
    "    datastr=[]\n",
    "    for i in range(len(Conthour)):\n",
    "        continent=Conthour['continent'][i]\n",
    "        hour=Conthour['hour'][i]\n",
    "        contIndex=uniqueCont.index(continent)\n",
    "        outMatrix[contIndex][int(hour)]=Conthour['sentiment'][i]\n",
    "        \n",
    "        \n",
    "    for i in range(numCont):\n",
    "        for j in range(numHour):\n",
    "            datastr.append(\"[\"+str(j)+\",\"+str(i)+\",\"+str(int(outMatrix[i][j]))+\"]\")\n",
    "\n",
    "\n",
    "    outputStr+=\",\".join(datastr)+\"]; var minval = \"+str(minVal)+\";\\n var maxval = \"+str(maxVal)+\";\"\n",
    "    \n",
    "    return outputStr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-398b2a205968>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#d = path.dirname(__file__)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "#Wordcloud:\n",
    "\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "    #d = path.dirname(__file__)\n",
    "def createwordcloud(tweets):  \n",
    "    # Read the whole text.\n",
    "    #text = open(path.join(d, 'constitution.txt')).read()\n",
    "    textpos = tweets[tweets.sentiment_cat == 'Pos']\n",
    "    textneg = tweets[tweets.sentiment_cat == 'Neg']\n",
    "    \n",
    "    postweets=\"\"\n",
    "    for i in textpos.index.values:\n",
    "        postweets+=textpos['text'][i]+\" \"\n",
    "    negtweets=\"\"\n",
    "    for i in textneg.index.values:\n",
    "        negtweets+=textneg['text'][i]+\" \"\n",
    "    \n",
    "    textp = preprocess(postweets)\n",
    "    textp=\" \".join(textp)\n",
    "    textn = preprocess(negtweets)\n",
    "    textn=\" \".join(textn)\n",
    "    wordcloudp = WordCloud( stopwords=stop,background_color='white',width=1200,height=1000).generate(textp)\n",
    "    wordcloudn = WordCloud( stopwords=stop,background_color='white', width=1200,height=1000).generate(textn)\n",
    "    image1 = wordcloudp.to_image()\n",
    "    image2= wordcloudn.to_image()\n",
    "    image1.save(\"wordcloup.png\")\n",
    "    image2.save(\"wordcloudn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sentiment matrix:\n",
    "\n",
    "td1 = pd.DataFrame({'value' : tweets.groupby( [ \"created_at\"],['country'],['country_code'] ).size()}).reset_index()\n",
    "timedata = td1[td1.created_at != 'NA']\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
