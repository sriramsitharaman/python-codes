{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\gensim-1.0.1-py3.5-win-amd64.egg\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-06-18 18:26:19,046 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-18 18:26:35,928 : INFO : loading projection weights from C:\\Users\\sriram\\Downloads\\GoogleNews-vectors-negative300.bin.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-18 18:29:23,712 : INFO : loaded (3000000, 300) matrix from C:\\Users\\sriram\\Downloads\\GoogleNews-vectors-negative300.bin.gz\n",
      "2017-06-18 18:29:23,712 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"C:\\\\Users\\\\sriram\\\\Downloads\\\\GoogleNews-vectors-negative300.bin.gz\",\n",
    "    binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Haccker Earth\\\\\"\n",
    "train = pd.read_csv(path+\"train.csv\", sep=',', quotechar='\"')\n",
    "train=train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(300,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-18 18:32:45,595 : WARNING : cannot compute similarity with no input ['Blurby']\n",
      "2017-06-18 18:32:45,762 : WARNING : cannot compute similarity with no input ['www.theimpossiblegirl.com']\n",
      "2017-06-18 18:32:48,730 : WARNING : cannot compute similarity with no input ['AhhhhhhhhhhHh']\n",
      "2017-06-18 18:32:49,261 : WARNING : cannot compute similarity with no input ['www.foldersnacks.com']\n",
      "2017-06-18 18:32:50,153 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:32:51,342 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:32:57,744 : WARNING : cannot compute similarity with no input ['ARELLA']\n",
      "2017-06-18 18:33:00,541 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:01,938 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:02,111 : WARNING : cannot compute similarity with no input ['n/a']\n",
      "2017-06-18 18:33:03,229 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:03,879 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:03,935 : WARNING : cannot compute similarity with no input ['EWBANH']\n",
      "2017-06-18 18:33:05,070 : WARNING : cannot compute similarity with no input ['...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...']\n",
      "2017-06-18 18:33:06,304 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:09,008 : WARNING : cannot compute similarity with no input ['www.artistswomen.com']\n",
      "2017-06-18 18:33:12,728 : WARNING : cannot compute similarity with no input ['--']\n",
      "2017-06-18 18:33:12,861 : WARNING : cannot compute similarity with no input ['needtogetavantospreadourhappymusictotheworld']\n",
      "2017-06-18 18:33:13,197 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:13,640 : WARNING : cannot compute similarity with no input ['...']\n",
      "2017-06-18 18:33:14,980 : WARNING : cannot compute similarity with no input ['Fantasy/action/drama']\n",
      "2017-06-18 18:33:16,100 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:18,624 : WARNING : cannot compute similarity with no input ['www.angryyoungwomen.net', 'AYWSeries', 'Facebook/AngryYoungWomen', 'angryyoungwomen', 'kadm.com']\n",
      "2017-06-18 18:33:19,462 : WARNING : cannot compute similarity with no input ['...']\n",
      "2017-06-18 18:33:19,594 : WARNING : cannot compute similarity with no input ['renegadedrum.bandcamp.com']\n",
      "2017-06-18 18:33:21,140 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:22,627 : WARNING : cannot compute similarity with no input []\n",
      "2017-06-18 18:33:24,273 : WARNING : cannot compute similarity with no input ['Infideling']\n",
      "2017-06-18 18:33:29,498 : WARNING : cannot compute similarity with no input ['rea\\\\GHERGHRHHDEHHRHTHTJZA\\\\TJSTRJTRHJTNBDFDF']\n",
      "2017-06-18 18:33:30,688 : WARNING : cannot compute similarity with no input []\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_tokenized = train.apply(lambda x: w2v_tokenize_text(x['desc']), axis=1).values\n",
    "train_tokenized_1 = train.apply(lambda x: x['keywords'].split(\"-\"), axis=1).values\n",
    "train_tokenized_2 = train.apply(lambda x: w2v_tokenize_text(x['name']), axis=1).values\n",
    "X_train_desc_average = word_averaging_list(wv,train_tokenized)\n",
    "X_train_keywords_average = word_averaging_list(wv,train_tokenized_1)\n",
    "X_train_name_average = word_averaging_list(wv,train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df1=pd.DataFrame(X_train_desc_average)\n",
    "df2=pd.DataFrame(X_train_keywords_average)\n",
    "df3=pd.DataFrame(X_train_name_average)\n",
    "df1.columns=[\"desc_\"+str(i) for i in range(300)]\n",
    "df2.columns=[\"key_\"+str(i) for i in range(300)]\n",
    "df3.columns=[\"name_\"+str(i) for i in range(300)]\n",
    "dfFinal=pd.concat([df1, df2,df3], axis=1)\n",
    "\n",
    "del train_tokenized\n",
    "del train_tokenized_1\n",
    "del train_tokenized_2\n",
    "del X_train_desc_average\n",
    "del X_train_keywords_average\n",
    "del X_train_name_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['created_date']=train['created_at'].apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "train['deadline_date']=train['deadline'].apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "train['state_changed_date']=train['state_changed_at'].apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "train['created_day']=train['created_date'].apply(lambda x: int(x.day))\n",
    "train['created_month']=train['created_date'].apply(lambda x: int(x.month))\n",
    "train['created_year']=train['created_date'].apply(lambda x: int(x.year))\n",
    "train['created_hour']=train['created_date'].apply(lambda x: int(x.hour))\n",
    "train['deadline_diff']=(train['deadline_date']-train['created_date']).apply(lambda x: x.total_seconds()/3600/24)\n",
    "train['state_diff']=(train['state_changed_date']-train['created_date']).apply(lambda x: x.total_seconds()/3600/24)\n",
    "\n",
    "train_1=train[['project_id','goal','disable_communication','country','currency','backers_count'\n",
    "               , 'created_day', 'created_month', 'created_year',\n",
    "       'created_hour', 'deadline_diff', 'state_diff']]\n",
    "\n",
    "trainFinal=pd.concat([train_1, dfFinal], axis=1)\n",
    "\n",
    "del train['created_at']\n",
    "del train['deadline']\n",
    "del train['state_changed_at']\n",
    "del train['created_date']\n",
    "del train['deadline_date']\n",
    "del train['state_changed_date']\n",
    "del train['launched_at']\n",
    "\n",
    "del df1\n",
    "del df2\n",
    "del df3\n",
    "del dfFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Getting Categorical columns\n",
    "train_obj = trainFinal[['disable_communication','country','currency','created_day' ,'created_month', 'created_year' ,'created_hour']]\n",
    "\n",
    "train_obj['created_day'] = train_obj['created_day'].astype(object)\n",
    "train_obj['created_month'] = train_obj['created_month'].astype(object)\n",
    "train_obj['created_year'] = train_obj['created_year'].astype(object)\n",
    "train_obj['created_hour'] = train_obj['created_hour'].astype(object)\n",
    "train_obj_dum = pd.get_dummies(train_obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainFinal.drop(train_obj.columns.values,axis=1,inplace=True)\n",
    "trainFinal=pd.concat([trainFinal.ix[:,:904], train_obj,train['final_status']], axis=1)\n",
    "trainFinal.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
