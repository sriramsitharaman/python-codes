{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\Train\\\\\"\n",
    "inputFile=\"Train-Hilary\"\n",
    "from io import StringIO\n",
    "#inp=open(folder+inputFile+\".csv\")\n",
    "\n",
    "with open(folder+inputFile+\".csv\", encoding='ISO-8859-1') as file_obj:\n",
    "    reader = csv.reader(file_obj,skipinitialspace=True)\n",
    "    outputFile=open(folder+\"Words\\\\\"+inputFile+\".txt\",\"wb\")\n",
    "    for row in reader:\n",
    "        words=preprocess(row[0])\n",
    "        #print(words)    \n",
    "        for word in words:\n",
    "            #print (str.encode(word),type(str.encode(word)))\n",
    "            outputFile.write(str.encode(word+\"\\n\"))\n",
    "        outputFile.write(str.encode(\"\\n\"))\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    print (tokens)\n",
    "    tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tweet = 'http://tiny.fg @tedcruz And, #HandOverTheServer she wiped clean + 30k deleted emails, explains dereliction of duty/lies re #Benghazi,etc #tcot'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "outputFile=open(folder+\"WordVector.txt\",\"w\")\n",
    "with open(folder+inputFile, encoding='ISO-8859-1') as file_obj:\n",
    "    reader = csv.reader(file_obj,skipinitialspace=True)\n",
    "    for row in reader:\n",
    "        words=preprocess(row[0])\n",
    "        print (words)\n",
    "        LowerWords=[x.lower() for x in words]\n",
    "        #print(words)\n",
    "        cols=[]\n",
    "        wordList=open(folder+\"FinalWords.txt\")\n",
    "        with open(folder+\"FinalWords.txt\", encoding='ISO-8859-1') as wordFile_obj:\n",
    "            for col in wordFile_obj:\n",
    "                if col.replace(\"\\n\",\"\").lower() in LowerWords:\n",
    "                    cols.append(1)\n",
    "                    #print (\"hi\")\n",
    "                else:\n",
    "                    cols.append(0)\n",
    "            outputFile.write(\"|\".join([str(x) for x in cols])+\"\\n\")\n",
    "        count+=1\n",
    "        if count%50==0:\n",
    "            print (count)\n",
    "\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\Train\\\\Words\\\\POSTagged-Words\\\\\"\n",
    "onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "for file in onlyfiles:\n",
    "    LineCount=0\n",
    "    print(file)\n",
    "    out=open(folder+\"Word-Tags\\\\Output-\"+file,\"w\")\n",
    "    out2=open(folder+\"Word-Tags\\\\Output-Final-\"+file,\"w\")\n",
    "    out.write(\"Word|Tag\\n\")\n",
    "    inp=open(folder+file)\n",
    "    #file_obj = StringIO(inp.read().decode())\n",
    "    tagDict=dict()\n",
    "    with open(folder+file,\"rt\") as file_obj:\n",
    "        reader = csv.reader(file_obj,skipinitialspace=True,delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            print (row)\n",
    "            if LineCount>11:\n",
    "                if len(row)>0:\n",
    "                    line=(\"|\".join(row[0].split())).replace(\"\\n\",\"\")\n",
    "                    out.write(line+\"\\n\")\n",
    "                    tag=(row[0]).replace(\"\\n\",\"\")\n",
    "                    for i in range(1,6):\n",
    "                        if len(row[i])>0:\n",
    "                            tag=row[i]\n",
    "                            break\n",
    "                    word=(row[0]).replace(\"\\n\",\"\")\n",
    "                    if tag[0]==\"N\" or tag[0]==\"V\" or tag[0]==\"R\":\n",
    "                        if word in tagDict and tag not in tagDict[word]:\n",
    "                            tagDict[word].append(tag)\n",
    "                        else:\n",
    "                            tagDict[word]=[tag]\n",
    "        \n",
    "            LineCount+=1\n",
    "    print (tagDict)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Arguing lexicon\n",
    "#Creating for total dictionary for Macros\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pprint import pprint\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\arglex_Somasundaran07\\\\Macros\\\\\"\n",
    "onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "#macros=[]\n",
    "#for file in onlyfiles:\n",
    "with open(folder+\"TotalMacro.json\") as data_file:    \n",
    "    macro=json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\arglex_Somasundaran07\\\\\"\n",
    "onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "FeautureRegexList=[]\n",
    "import re\n",
    "for file in onlyfiles:\n",
    "    LineCount=0\n",
    "    inp=open(folder+file)\n",
    "    regexList=[]\n",
    "    for line in inp:\n",
    "        if LineCount>0:\n",
    "            line=line.replace(\"\\n\",\"\")\n",
    "            #print (re.sub(r\"\\\\\\\\\", r\"\\\\\",line))\n",
    "            regexList.append(re.sub(r\"\\\\\\\\\", r\"\",line))\n",
    "        LineCount+=1\n",
    "    inp.close()\n",
    "    #break\n",
    "    FeautureRegexList.append(regexList)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=FeautureRegexList[7][4]\n",
    "p=re.compile(x,re.IGNORECASE)\n",
    "print (p)\n",
    "#m=re.match(FeautureRegexList[7][4],\"it's pretty evident that\")\n",
    "#print (m)\n",
    "#if m:\n",
    "#   print (True)\n",
    "if p.match(\"it's pretty evident that\"):\n",
    "    print (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replacemacro=[5,7,11,12,14]\n",
    "replacemacro=[]\n",
    "for i in range(len(FeautureRegexList)):\n",
    "    replaceList=FeautureRegexList[i]\n",
    "    temp=[]\n",
    "    for regx in replaceList:\n",
    "        lenCheck=0\n",
    "        print (regx)\n",
    "        atCheck=re.findall(\"\\(@[A-Z,0-9]* ?\\)\",regx)\n",
    "        regxMacros=[i.replace(\"(\",\"\").replace(\")\",\"\") for i in atCheck]\n",
    "        if len(atCheck)>0:\n",
    "            lenCheck=1\n",
    "        else:\n",
    "            lenCheck=0\n",
    "        newRegx=regx\n",
    "        for mac in regxMacros:\n",
    "            replaceString=\" |\".join(macro[mac.replace(\" \",\"\")])\n",
    "            mac=\"(\"+mac+\")\"\n",
    "            newRegx=newRegx.replace(mac,\"(\"+replaceString+ \" )\")\n",
    "        if lenCheck==1:\n",
    "            temp.append(newRegx.replace(\") \",\")\").replace(\"\\\"\",\"\\'\"))\n",
    "        else:\n",
    "            temp.append(regx)\n",
    "    replacemacro.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\Train\\\\\"\n",
    "inputFile=\"Train-Hilary\"\n",
    "from io import StringIO\n",
    "#inp=open(folder+inputFile+\".csv\")\n",
    "\n",
    "with open(folder+inputFile+\".csv\", encoding='ISO-8859-1') as file_obj:\n",
    "    reader = csv.reader(file_obj,skipinitialspace=True)\n",
    "    outputFile=open(folder+\"Words\\\\\"+inputFile+\".txt\",\"wb\")\n",
    "    for row in reader:\n",
    "        words=preprocess(row[0])\n",
    "        #print(words)    \n",
    "        for word in words:\n",
    "            #print (str.encode(word),type(str.encode(word)))\n",
    "            outputFile.write(str.encode(word+\"\\n\"))\n",
    "        outputFile.write(str.encode(\"\\n\"))\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\\\Train\\\\\"\n",
    "argFolder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\arglex_Somasundaran07\\\\\"\n",
    "onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "ArgFeatures=[f.replace(\".tff\",\"\") for f in listdir(argFolder) if '.tff' in f]\n",
    "header=\",\".join(ArgFeatures)\n",
    "print(len(replacemacro))\n",
    "for file in onlyfiles:\n",
    "    LineCount=0\n",
    "    rowCount=0\n",
    "    out=open(folder+\"ArgLex Features\\\\ArgFeatures-\"+file,\"w\")\n",
    "    out.write(header+\"\\n\")\n",
    "    with open(folder+file,\"rt\", encoding='ISO-8859-1') as file_obj:\n",
    "        reader = csv.reader(file_obj,skipinitialspace=True)\n",
    "        for row in reader:\n",
    "            if rowCount>0:    \n",
    "                tweet=row[0].replace(\"\\n\",\"\")\n",
    "                Featurevalues=[]\n",
    "                for i in range(len(replacemacro)):\n",
    "                    currentFeature=replacemacro[i]\n",
    "                    match=0\n",
    "                    for regx in currentFeature:\n",
    "                        p=re.compile(regx,re.IGNORECASE)\n",
    "                        if p.match(tweet) and len(regx)>1:\n",
    "                            #print (regx)\n",
    "                            #print (tweet)\n",
    "                            match+=1\n",
    "                            break\n",
    "                    Featurevalues.append(match)\n",
    "                out.write(\",\".join([str(i) for i in Featurevalues])+\"\\n\")\n",
    "            rowCount+=1\n",
    "    print (file)\n",
    "    out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replacemacro[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "outputFile=open(folder+\"WordVector-Test.txt\",\"w\")\n",
    "with open(folder+\"Athesim-Test.csv\", encoding='ISO-8859-1') as file_obj:\n",
    "    reader = csv.reader(file_obj,skipinitialspace=True)\n",
    "    for row in reader:\n",
    "        words=preprocess(row[0])\n",
    "        print (words)\n",
    "        LowerWords=[x.lower() for x in words]\n",
    "        #print(words)\n",
    "        cols=[]\n",
    "        wordList=open(folder+\"FinalWords.txt\")\n",
    "        with open(folder+\"FinalWords.txt\", encoding='ISO-8859-1') as wordFile_obj:\n",
    "            for col in wordFile_obj:\n",
    "                if col.replace(\"\\n\",\"\").lower() in LowerWords:\n",
    "                    cols.append(1)\n",
    "                    #print (\"hi\")\n",
    "                else:\n",
    "                    cols.append(0)\n",
    "            outputFile.write(\"|\".join([str(x) for x in cols])+\"\\n\")\n",
    "        count+=1\n",
    "        if count%50==0:\n",
    "            print (count)\n",
    "\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Assignments\\\\Assignment 4\\\\\"\n",
    "train = pd.read_csv(folder+'AtheismWords-Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'Stance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse import stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initialized\n",
      "0\n",
      "California US\n",
      "Texas US\n",
      "New York US\n",
      "Northwest Territories CA\n",
      "Florida US\n",
      "MT BR\n",
      "Washington US\n",
      "Florida US\n",
      "Oklahoma US\n",
      "Minnesota US\n",
      "Kentucky US\n",
      "New Jersey US\n",
      "Indiana US\n",
      "Kansas US\n",
      "Alabama US\n",
      "California US\n",
      "Kentucky US\n",
      "Yucatán MX\n",
      "England GB\n",
      "Texas US\n",
      "England GB\n",
      "Ohio US\n",
      "Gelderland NL\n",
      "دبي‎ AE\n",
      "California US\n",
      "Nova Scotia CA\n",
      "Mecklenburg-Vorpommern DE\n",
      "England GB\n",
      "ຫົວພັນ LA\n",
      "England GB\n",
      "Nevada US\n",
      "Texas US\n",
      "Texas US\n",
      "Madhya Pradesh IN\n",
      "Ticino CH\n",
      "England GB\n",
      "New Jersey US\n",
      "Kentucky US\n",
      "England GB\n",
      "Western Australia AU\n",
      "Maharashtra IN\n",
      "MT BR\n",
      "Colorado US\n",
      "Pennsylvania US\n",
      "پنجاب PK\n",
      "Michigan US\n",
      "England GB\n",
      "Texas US\n",
      "England GB\n",
      "England GB\n",
      "Bali ID\n",
      "California US\n",
      "Washington US\n",
      "Kampala UG\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "path='F:\\\\World Tweets 04_14_15\\\\'\n",
    "tweetFile='WorldTweets-'\n",
    "import pandas as pd\n",
    "#import zipcode\n",
    "#from textblob import TextBlob\n",
    "#from random import shuffle\n",
    "import sys\n",
    "tweets_data = []\n",
    "fileno=5\n",
    "tweets_data_path=path+tweetFile+str(fileno)+\".txt\"\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "\ttry:\n",
    "\t\ttweet = json.loads(line)\n",
    "\t\ttweets_data.append(tweet)\n",
    "\texcept:\n",
    "\t\tcontinue\n",
    "\n",
    "tweets = pd.DataFrame(index=range(len(tweets_data)), columns=['text','lang','created_at','hour','retweet','country_code','state','sentiment','lat','lon'])\n",
    "print (\"dataset initialized\")\t\n",
    "count=0\n",
    "#shuffle(data)\n",
    "#tweets_data=data[:200]\n",
    "for i in range(len(tweets_data)):\n",
    "\ttry:\n",
    "\t\ttweets['text'][i] = tweets_data[i]['text']\n",
    "\texcept:\n",
    "\t\ttweets['text'][i] = \"\"\n",
    "\ttry:\n",
    "\t\ttweets['lang'][i]=tweets_data[i]['lang']\n",
    "\texcept:\n",
    "\t\ttweets['lang'][i]='NA'\n",
    "\ttry:\n",
    "\t\ttweets['retweet'][i]='0' if \"RT @\" not in tweets['text'][i] else '1'\n",
    "\texcept:\n",
    "\t\ttweets['lang'][i]='NA'\n",
    "\ttry:\n",
    "\t\ttweets['country_code'][i]=str(tweets_data[i]['place']['country_code']).upper()\n",
    "\texcept:\n",
    "\t\ttweets['country_code'][i]=''\n",
    "\ttry:\n",
    "\t\ttweets['lon'][i]=tweets_data[i]['place']['bounding_box']['coordinates'][0][0][0]\n",
    "\texcept:\n",
    "\t\ttweets['lon'][i]='NA'\n",
    "\ttry:\n",
    "\t\ttweets['lat'][i]=tweets_data[i]['place']['bounding_box']['coordinates'][0][0][1]\n",
    "\texcept:\n",
    "\t\ttweets['lat'][i]='NA'\n",
    "\ttry:\n",
    "\t\ttweets['hour'][i]=tweets_data[i]['created_at'][11:13]\n",
    "\texcept:\n",
    "\t\ttweets['hour'][i]='NA'\n",
    "\ttry:\n",
    "\t\ttweets['created_at'][i]=tweets_data[i]['created_at']\n",
    "\texcept:\n",
    "\t\ttweets['created_at'][i]='NA'\n",
    "#\tblob = TextBlob(tweets['text'][i])\n",
    "#\ttry:\n",
    "#\t\tsentence=blob.sentences[0]\n",
    "#\t\ttweets['sentiment'][i]=sentence.sentiment.polarity\n",
    "#\texcept:\n",
    "#\t\ttweets['sentiment'][i]=0\n",
    "\ttry:\n",
    "\t\tlocation=tweets_data[i]['user']['location']\n",
    "\texcept:\n",
    "\t\tlocation=\"NA\"\n",
    "\tif location is not None and location!=\"NA\":\n",
    "\t\ttry:\n",
    "\t\t\tcoor=geolocator.geocode(location)\n",
    "\t\t\tcoor=coor.raw\n",
    "\t\t\taddress = geolocator.reverse(coor['lat']+\",\"+coor['lon']).raw\n",
    "\t\t\tcountry_code=str(address['address']['country_code']).upper()\n",
    "\t\t\ttweets['country_code'][i]=country_code\n",
    "\t\t\ttweets['state'][i]=address['address']['state']\n",
    "\t\t\t#print (address['address']['state'],country_code)\n",
    "\t\texcept:\n",
    "\t\t\tcountry_code=\"\"\n",
    "\t\t\ttweets['country_code'][i]=\"\"\n",
    "\t\t\ttweets['state'][i]=\"\"\n",
    "\t#if len(str(tweets['country_code']))<=1:\n",
    "\t#\ttweets['country_code']=country_code\n",
    "\t#try:\n",
    "\t#\tstateFromData=tweets['location'][i].split(',')[1]\n",
    "\t#except:\n",
    "\t#\tstateFromData=''\n",
    "\t#if len(stateFromData)==2:\n",
    "\t#\ttweets['state'][i]=stateFromData\n",
    "\t#else:\n",
    "\t#\tif tweets['lat'][i] !='NA':\n",
    "\t#\t\tradius=10\n",
    "\t#\t\tincre=10\n",
    "\t#\t\tzips=zipcode.isinradius((tweets['lat'][i],tweets['lon'][i]),radius)\n",
    "\t#\t\twhile len(zips)==0:\n",
    "\t#\t\t\tradius=radius+incre\n",
    "\t#\t\t\tzips=zipcode.isinradius((tweets['lat'][i],tweets['lon'][i]),radius)\n",
    "\t#\t\t\tincre=incre+10\n",
    "\t#\t\tmyzip = zipcode.isequal(str(zips[0].zip))\n",
    "\t#\t\ttweets['state'][i]=myzip.state\n",
    "\t#\telse:\n",
    "\t#\t\ttweets['state'][i]='NA'\t\n",
    "\t\n",
    "\tif i%400==0:\n",
    "\t\tprint (i)\n",
    "\n",
    "\n",
    "tweets.to_csv(path+\"tweets-output/\"+tweetFile+str(fileno)+\".csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "path=''\n",
    "tweetFile='WorldTweets-'\n",
    "import pandas as pd\n",
    "#import zipcode\n",
    "from textblob import TextBlob\n",
    "#from random import shuffle\n",
    "import sys\n",
    "tweets_data = []\n",
    "fileno=sys.argv[1]\n",
    "tweets_data_path=path+tweetFile+str(fileno)+\".txt\"\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "\ttry:\n",
    "\t\ttweet = json.loads(line)\n",
    "\t\ttweets_data.append(tweet)\n",
    "\texcept:\n",
    "\t\tcontinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "import string\n",
    "for i in range(len(tweets_data)):\n",
    "    try:\n",
    "        location=tweets_data[i]['user']['location']\n",
    "    except:\n",
    "        location=\"NA\"\n",
    "    if location is not None and location!=\"NA\":\n",
    "        try:\n",
    "            coor=geolocator.geocode(location)\n",
    "            coor=coor.raw\n",
    "            address = geolocator.reverse(coor['lat']+\",\"+coor['lon']).raw\n",
    "            country_code=address['address']['country_code']\n",
    "            country=address['address']['country']\n",
    "            count+=1\n",
    "            print (i+1,count,country_code,location)\n",
    "        except:\n",
    "            print (coor)\n",
    "            print ('NA NA',location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coor=geolocator.geocode(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coor.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coor=geolocator.geocode('California United States of America')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coor=coor.raw\n",
    "address = geolocator.reverse(coor['lat']+\",\"+coor['lon']).raw\n",
    "country_code=address['address']['country_code']\n",
    "country=address['address']['country']\n",
    "print (country, country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Spring 2017\\\\Big Data\\\\I524 Project\\\\WorldTweets-5.csv\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tcData=pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hour</th>\n",
       "      <th>retweet</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @JasonKander: Actual poll on https://t.co/3...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fri Apr 14 18:46:17 +0000 2017</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @geewaxnpr: Ethics experts say #Trump must ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fri Apr 14 18:46:17 +0000 2017</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@farzyness @AlexRoy144 I'm not sure you unders...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fri Apr 14 18:46:17 +0000 2017</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I just claimed my Daily Bonus at 2020: My Coun...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fri Apr 14 18:46:17 +0000 2017</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                               text lang  \\\n",
       "0          0  RT @JasonKander: Actual poll on https://t.co/3...   en   \n",
       "1          1                                                NaN  NaN   \n",
       "2          2  RT @geewaxnpr: Ethics experts say #Trump must ...   en   \n",
       "3          3  @farzyness @AlexRoy144 I'm not sure you unders...   en   \n",
       "4          4  I just claimed my Daily Bonus at 2020: My Coun...   en   \n",
       "\n",
       "                       created_at  hour  retweet country_code  state  \\\n",
       "0  Fri Apr 14 18:46:17 +0000 2017  18.0      1.0          NaN    NaN   \n",
       "1                             NaN   NaN      0.0          NaN    NaN   \n",
       "2  Fri Apr 14 18:46:17 +0000 2017  18.0      1.0          NaN    NaN   \n",
       "3  Fri Apr 14 18:46:17 +0000 2017  18.0      0.0          NaN    NaN   \n",
       "4  Fri Apr 14 18:46:17 +0000 2017  18.0      0.0          NaN    NaN   \n",
       "\n",
       "   sentiment  lat  lon  \n",
       "0   0.142857  NaN  NaN  \n",
       "1   0.000000  NaN  NaN  \n",
       "2   0.000000  NaN  NaN  \n",
       "3  -0.250000  NaN  NaN  \n",
       "4   0.000000  NaN  NaN  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>hour</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AU</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BH</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BR</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CA</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CR</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EG</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FR</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GB</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IL</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KE</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KW</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MX</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NG</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NL</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PH</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PK</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PL</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SG</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TR</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>US</td>\n",
       "      <td>18.0</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ZA</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_code  hour  value\n",
       "0            AE  18.0      1\n",
       "1            AR  18.0      1\n",
       "2            AU  18.0      1\n",
       "3            BH  18.0      1\n",
       "4            BR  18.0      2\n",
       "5            CA  18.0      5\n",
       "6            CR  18.0      1\n",
       "7            EG  18.0      1\n",
       "8            FR  18.0      1\n",
       "9            GB  18.0     29\n",
       "10           IL  18.0      1\n",
       "11           IN  18.0      3\n",
       "12           KE  18.0      1\n",
       "13           KW  18.0      3\n",
       "14           MX  18.0      1\n",
       "15           NG  18.0      1\n",
       "16           NL  18.0      1\n",
       "17           PH  18.0      1\n",
       "18           PK  18.0      1\n",
       "19           PL  18.0      1\n",
       "20           SG  18.0      2\n",
       "21           TR  18.0      3\n",
       "22           US  18.0    132\n",
       "23           ZA  18.0      6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=pd.DataFrame({'value' : tcData.groupby( [ \"country_code\",\"hour\"] ).size()}).reset_index()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "def getplace(lat, lon):\n",
    "    url = \"http://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "    url += \"latlng=%s,%s&sensor=false\" % (lat, lon)\n",
    "    v = urllib.request.urlopen(url).read()\n",
    "    j = str(v).json()\n",
    "    components = j['results'][0]['address_components']\n",
    "    country = town = None\n",
    "    for c in components:\n",
    "        if \"country\" in c['types']:\n",
    "            country = c['long_name']\n",
    "        if \"postal_town\" in c['types']:\n",
    "            town = c['long_name']\n",
    "    return town, country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def getplace(lat, lon):\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    params = {'latlng': str(lat)+\",\"+str(lon)}\n",
    "    r = requests.get(url, params=params)\n",
    "    results = r.json()['results']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.7441258430481\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=time.time()\n",
    "coor=geolocator.geocode(location)\n",
    "coor=coor.raw\n",
    "address = geolocator.reverse(coor['lat']+\",\"+coor['lon']).raw\n",
    "print (time.time()-x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
