{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "labels = {'pos':1, 'neg':0,'other':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TrainData\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\Train\\\\\"\n",
    "file=\"train.csv\"\n",
    "df=pd.DataFrame()\n",
    "Count=0\n",
    "with open(os.path.join(folder, file), 'r', encoding='ISO-8859-1') as infile:\n",
    "    reader = csv.reader(infile,skipinitialspace=True)\n",
    "    for row in reader:\n",
    "        if Count>0:\n",
    "            df = df.append([[row[0],row[1],labels[row[4]]]], ignore_index=True)\n",
    "        Count+=1\n",
    "df.columns = ['Tweet','Target', 'Stance']\n",
    "arguTrain=pd.read_csv(folder+\"ArgLex Features\\\\\"+\"ArgFeatures-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FinalDf=pd.concat([df.reset_index(drop=True), arguTrain.reset_index(drop=True)], axis=1)\n",
    "df=FinalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TestData\n",
    "folder=\"C:\\\\Users\\\\sriram\\\\OneDrive\\\\Computational Linguistics\\\\Final Project\\\\Targets\\\\Test\\\\\"\n",
    "file=\"test.csv\"\n",
    "dfTest=pd.DataFrame()\n",
    "Count=0\n",
    "with open(os.path.join(folder, file), 'r', encoding='ISO-8859-1') as infile:\n",
    "    reader = csv.reader(infile,skipinitialspace=True)\n",
    "    for row in reader:\n",
    "        if Count>0:\n",
    "            dfTest = dfTest.append([[row[0],row[1],labels[row[4]]]], ignore_index=True)\n",
    "        Count+=1\n",
    "dfTest.columns = ['Tweet','Target', 'Stance']\n",
    "arguTest=pd.read_csv(folder+\"ArgLex Features\\\\\"+\"ArgFeatures-test.csv\")\n",
    "FinalDftest=pd.concat([dfTest.reset_index(drop=True), arguTest.reset_index(drop=True)], axis=1)\n",
    "\n",
    "dfTest=FinalDftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1956, 20)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating Separate datasets for each Target\n",
    "HillaryTrain=df.loc[df.Target==\"Hillary Clinton\"]\n",
    "AtheismTrain=df.loc[df.Target==\"Atheism\"]\n",
    "AbortionTrain=df.loc[df.Target==\"Legalization of Abortion\"]\n",
    "FeministTrain=df.loc[df.Target==\"Feminist Movement\"]\n",
    "ClimateTrain=df.loc[df.Target==\"Climate Change is a Real Concern\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating Separate datasets for each Target\n",
    "HillaryTest=dfTest.loc[dfTest.Target==\"Hillary Clinton\"]\n",
    "AtheismTest=dfTest.loc[dfTest.Target==\"Atheism\"]\n",
    "AbortionTest=dfTest.loc[dfTest.Target==\"Legalization of Abortion\"]\n",
    "FeministTest=dfTest.loc[dfTest.Target==\"Feminist Movement\"]\n",
    "ClimateTest=dfTest.loc[dfTest.Target==\"Climate Change is a Real Concern\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming documents into feature vectors\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining and the weather is sweet'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'and': 0, 'sweet': 4, 'the': 5, 'sun': 3, 'weather': 6, 'shining': 2}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "#Assessing word relevancy via term frequency-inverse document frequency\n",
    "np.set_printoptions(precision=2)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sriram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from utils import *\n",
    "stop = stopwords.words('english')\n",
    "print (stop)\n",
    "[w for w in tokenizer('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':-)', 'Happy', ':-(', 'Sad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\":-)Happy :-( Sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hillary is our best choice if we truly want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@TheView I think our country is ready for a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just gave an unhealthy amount of my hard-ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@PortiaABoulger Thank you for adding me to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hillary can not win. Here's hoping the Dems of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet\n",
       "0  Hillary is our best choice if we truly want to...\n",
       "1  @TheView I think our country is ready for a fe...\n",
       "2  I just gave an unhealthy amount of my hard-ear...\n",
       "3  @PortiaABoulger Thank you for adding me to you...\n",
       "4  Hillary can not win. Here's hoping the Dems of..."
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"@TheView I think our country is ready for a female pres, it can't ever be Hillary\"\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "check=df.ix[:,0:1]\n",
    "check2=df.ix[:,3:4]\n",
    "print (np.concatenate([check,check2], axis=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, \n",
    "                        lowercase=False, \n",
    "                        preprocessor=None)\n",
    "\n",
    "#param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "#               'vect__stop_words': [stop],\n",
    "#               'vect__tokenizer': [tokenizer],\n",
    "#               'clf__penalty': ['l1', 'l2'],\n",
    "#               'clf__C': [1.0, 10.0, 100.0],\n",
    "#              'vect__max_features':[1500,1600,1700]}]\n",
    "#\n",
    "#param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "#               'vect__stop_words': [stop],\n",
    "#               'vect__tokenizer': [tokenizer],\n",
    "#               'clf__n_estimators': [10,15,20],\n",
    "#               'clf__min_samples_leaf':[5,10],\n",
    "#              'vect__max_features':[500,750,1000,1250,1500,1750,2000]},\n",
    "#             {'vect__ngram_range': [(1,1)],\n",
    "#               'vect__stop_words': [stop],\n",
    "#               'vect__tokenizer': [tokenizer],\n",
    "#               'clf__n_estimators': [10,15,20,25,50,100],\n",
    "#               'clf__min_samples_leaf':[3,5,10],\n",
    "#              'vect__max_features':[500,750,1000,1250,1500,1750,2000,2500]}]\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'clf__n_estimators': [10,15,20,25,50,100],\n",
    "               'clf__min_samples_leaf':[5,10],\n",
    "              'vect__max_features':[500,750,1000,1250,1500,1750,2000,2500]}]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                    ('clf', RandomForestClassifier())])\n",
    "\n",
    "\n",
    "mh=mat=mab=mc=mf = GridSearchCV(lr_tfidf, param_grid, \n",
    "                           scoring='accuracy',\n",
    "                           cv=5, verbose=1,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 1, 4, 31, 43, 221]\n"
     ]
    }
   ],
   "source": [
    "a=[1,221,31,43,4,-1]\n",
    "a=sorted(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   48.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelHilary Best parameter set: {'vect__tokenizer': <function tokenizer at 0x000002AEF76C06A8>, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'clf__n_estimators': 10, 'vect__max_features': 500, 'clf__min_samples_leaf': 5, 'vect__ngram_range': (1, 1)} \n",
      "ModelHilary CV Accuracy: 0.682\n",
      "Test Accuracy: 0.692\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 343 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 473 out of 480 | elapsed:   23.9s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   24.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelAtheism Best parameter set: {'vect__tokenizer': <function tokenizer at 0x000002AEF76C06A8>, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'clf__n_estimators': 50, 'vect__ngram_range': (1, 1), 'clf__min_samples_leaf': 5, 'vect__max_features': 500} \n",
      "ModelAtheism CV Accuracy: 0.643\n",
      "Test Accuracy: 0.614\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   28.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelAbortion Best parameter set: {'vect__tokenizer': <function tokenizer at 0x000002AEF76C06A8>, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'clf__n_estimators': 20, 'vect__ngram_range': (1, 1), 'vect__max_features': 500, 'clf__min_samples_leaf': 5} \n",
      "ModelAbortion CV Accuracy: 0.672\n",
      "Test Accuracy: 0.696\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 345 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 473 out of 480 | elapsed:   22.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   22.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelClimate Best parameter set: {'vect__tokenizer': <function tokenizer at 0x000002AEF76C06A8>, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'vect__ngram_range': (1, 1), 'clf__n_estimators': 10, 'clf__min_samples_leaf': 5, 'vect__max_features': 750} \n",
      "ModelClimate CV Accuracy: 0.532\n",
      "Test Accuracy: 0.515\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   26.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelFeminist Best parameter set: {'vect__tokenizer': <function tokenizer at 0x000002AEF76C06A8>, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'clf__n_estimators': 10, 'vect__max_features': 500, 'clf__min_samples_leaf': 5, 'vect__ngram_range': (1, 1)} \n",
      "ModelFeminist CV Accuracy: 0.773\n",
      "Test Accuracy: 0.761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mh.fit(HillaryTrain['Tweet'].values,HillaryTrain.Stance)\n",
    "print('ModelHilary Best parameter set: %s ' % mh.best_params_)\n",
    "print('ModelHilary CV Accuracy: %.3f' % mh.best_score_)\n",
    "clf = mh.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(HillaryTest['Tweet'], HillaryTest.Stance))\n",
    "print()\n",
    "\n",
    "\n",
    "mat.fit(AtheismTrain['Tweet'].values,AtheismTrain.Stance)\n",
    "print('ModelAtheism Best parameter set: %s ' % mat.best_params_)\n",
    "print('ModelAtheism CV Accuracy: %.3f' % mat.best_score_)\n",
    "clf = mat.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(AtheismTest['Tweet'], AtheismTest.Stance))\n",
    "print()\n",
    "\n",
    "\n",
    "mab.fit(AbortionTrain['Tweet'].values,AbortionTrain.Stance)\n",
    "print('ModelAbortion Best parameter set: %s ' % mab.best_params_)\n",
    "print('ModelAbortion CV Accuracy: %.3f' % mab.best_score_)\n",
    "clf = mab.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(AbortionTest['Tweet'], AbortionTest.Stance))\n",
    "print()\n",
    "\n",
    "\n",
    "mc.fit(ClimateTrain['Tweet'].values,ClimateTrain.Stance)\n",
    "print('ModelClimate Best parameter set: %s ' % mc.best_params_)\n",
    "print('ModelClimate CV Accuracy: %.3f' % mc.best_score_)\n",
    "clf = mc.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(ClimateTest['Tweet'], ClimateTest.Stance))\n",
    "print()\n",
    "\n",
    "\n",
    "mf.fit(FeministTrain['Tweet'].values,FeministTrain.Stance)\n",
    "print('ModelFeminist Best parameter set: %s ' % mf.best_params_)\n",
    "print('ModelFeminist CV Accuracy: %.3f' % mf.best_score_)\n",
    "clf = mf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(FeministTest['Tweet'], FeministTest.Stance))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hillary Baseline Counter({0: 441, 1: 221, 2: 27}) 0.701694915254\n",
      "Abortion Baseline Counter({0: 432, 1: 188, 2: 33}) 0.721428571429\n",
      "Atheism Baseline Counter({1: 310, 0: 180, 2: 23}) 0.590909090909\n",
      "Climate Baseline Counter({0: 196, 1: 125, 2: 74}) 0.514792899408\n",
      "Feminist Baseline Counter({0: 513, 1: 119, 2: 32}) 0.761403508772\n"
     ]
    }
   ],
   "source": [
    "#Baseline\n",
    "from collections import Counter\n",
    "data = Counter(HillaryTrain.Stance)\n",
    "print(\"Hillary Baseline\",data,np.mean(HillaryTest.Stance==data.most_common(1)[0][0]))\n",
    "data = Counter(AbortionTrain.Stance)\n",
    "print(\"Abortion Baseline\",data,np.mean(AbortionTest.Stance==data.most_common(1)[0][0]))\n",
    "data = Counter(AtheismTrain.Stance)\n",
    "print(\"Atheism Baseline\",data,np.mean(AtheismTest.Stance==data.most_common(1)[0][0]))\n",
    "data = Counter(ClimateTrain.Stance)\n",
    "print(\"Climate Baseline\",data,np.mean(ClimateTest.Stance==data.most_common(1)[0][0]))\n",
    "data = Counter(FeministTrain.Stance)\n",
    "print(\"Feminist Baseline\",data,np.mean(FeministTest.Stance==data.most_common(1)[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [ 0.7  0.   0. ]\n",
      "recall: [ 1.  0.  0.]\n",
      "fscore: [ 0.82  0.    0.  ]\n",
      "support: [207  76  12]\n",
      "[[207   0   0]\n",
      " [ 76   0   0]\n",
      " [ 12   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "data = Counter(HillaryTrain.Stance)\n",
    "precision, recall, fscore, support=score(HillaryTest.Stance,[data.most_common(1)[0][0]]*HillaryTest.shape[0])\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print (confusion_matrix(HillaryTest.Stance,[data.most_common(1)[0][0]]*HillaryTest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [ 0.7  0.5  0. ]\n",
      "recall: [ 1.    0.01  0.  ]\n",
      "fscore: [ 0.82  0.03  0.  ]\n",
      "support: [207  76  12]\n",
      "[[206   1   0]\n",
      " [ 75   1   0]\n",
      " [ 12   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support=score(HillaryTest.Stance,clf.predict(HillaryTest['Tweet']))\n",
    "clf = mh.best_estimator_\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print (confusion_matrix(HillaryTest.Stance,clf.predict(HillaryTest['Tweet'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70169491525423733"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "207/sum(support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training a logistic regression model for document classification\n",
    "X_train = df.ix[:,0:1]\n",
    "y_train = df['Stance']\n",
    "X_test = dfTest.ix[:,0:1]\n",
    "y_test = dfTest['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#count_vect = CountVectorizer()\n",
    "#X_train_counts = count_vect.fit_transform(df['Tweet'].values.astype('U'))\n",
    "tf= TfidfVectorizer(strip_accents=None, \n",
    "                        lowercase=False, \n",
    "                        preprocessor=None,\n",
    "                        tokenizer=tokenizer11,\n",
    "                        max_features=1500)\n",
    "X_Train=tf.fit_transform(list(df['Tweet']))\n",
    "X_Test=tf.fit_transform(list(dfTest['Tweet']))\n",
    "#print(count_vect.vocabulary_.get(u'hilary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2914, 1507)\n",
      "(1956, 1507)\n"
     ]
    }
   ],
   "source": [
    "X_Train=X_Train.toarray()\n",
    "X_Train = np.append(X_Train, df.ix[:,3:10].values, 1)\n",
    "X_Test=X_Test.toarray()\n",
    "X_Test = np.append(X_Test, dfTest.ix[:,3:10].values, 1)\n",
    "print(X_Train.shape)\n",
    "print(X_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "all_data = Imputer().fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.    0.   ...,  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...,  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...,  0.    0.    0.  ]\n",
      " ..., \n",
      " [ 0.    0.    0.   ...,  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...,  0.    0.    0.  ]\n",
      " [ 0.23  0.    0.   ...,  0.    0.    0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=25, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "clf=LogisticRegression(random_state=0, multi_class='ovr',penalty='l2',C=1.0)\n",
    "clf.fit(X_Train,df.Stance)\n",
    "clf2 = RandomForestClassifier(n_estimators=25,min_samples_leaf=5)\n",
    "print (X_Train)\n",
    "clf2.fit(X_Train, df.Stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.626789366053\n",
      "0.609918200409\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(clf.predict(X_Test) == dfTest.Stance) )\n",
    "print(np.mean(clf2.predict(X_Test) == dfTest.Stance) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'vect__tokenizer': <function tokenizer at 0x000002AEF76C06A8>, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'clf__n_estimators': 10, 'vect__max_features': 750, 'vect__ngram_range': (1, 1), 'clf__min_samples_leaf': 5} \n",
      "CV Accuracy: 0.682\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.685\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68474576271186438"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf.predict(HillaryTest['Tweet'])==HillaryTest.Stance)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
